#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìœ íŠœë¸Œ ëŒ“ê¸€ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ê¸° v7.0
================================
PDF ë‹¤ìš´ë¡œë“œ + ë§ˆì¼€í„° í‚¤ì›Œë“œ ë¶„ì„
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import re
import io
import base64
from typing import List, Tuple, Optional
from collections import Counter
from datetime import datetime

# =============================================================================
# í˜ì´ì§€ ì„¤ì •
# =============================================================================
st.set_page_config(
    page_title="ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ê¸°",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# =============================================================================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# =============================================================================
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False
if 'clear_input' not in st.session_state:
    st.session_state.clear_input = False

# =============================================================================
# CSS
# =============================================================================
st.markdown("""
<style>
    .stApp { background-color: #f8fafc; }
    .block-container { padding: 2rem 3rem !important; max-width: 1100px !important; }
    
    .header { text-align: center; padding: 2rem 0 1.5rem 0; }
    .header h1 { color: #1e3a5f; font-size: 2rem; font-weight: 700; margin: 0 0 0.5rem 0; }
    .header p { color: #64748b; font-size: 1rem; margin: 0; }
    
    .card {
        background: white;
        border-radius: 12px;
        padding: 1.5rem;
        box-shadow: 0 1px 3px rgba(0,0,0,0.08);
        margin-bottom: 1rem;
    }
    
    .video-info-box {
        background: white;
        border-radius: 12px;
        padding: 1.5rem;
        box-shadow: 0 1px 3px rgba(0,0,0,0.08);
        margin-bottom: 1.5rem;
    }
    .video-info-row {
        display: flex;
        padding: 0.5rem 0;
        border-bottom: 1px solid #f1f5f9;
    }
    .video-info-row:last-child { border-bottom: none; }
    .video-info-label {
        color: #64748b;
        font-size: 0.85rem;
        min-width: 100px;
        font-weight: 500;
    }
    .video-info-value {
        color: #1e293b;
        font-size: 0.9rem;
        flex: 1;
    }
    
    .section-title {
        font-size: 1.1rem;
        font-weight: 600;
        color: #1e3a5f;
        margin: 2rem 0 1rem 0;
    }
    
    .insight {
        background: white;
        border-left: 3px solid #1e3a5f;
        padding: 1rem 1.25rem;
        margin-bottom: 0.75rem;
        border-radius: 0 8px 8px 0;
        box-shadow: 0 1px 3px rgba(0,0,0,0.08);
    }
    .insight-title { font-weight: 600; color: #1e3a5f; font-size: 0.95rem; margin-bottom: 0.4rem; }
    .insight-desc { color: #475569; font-size: 0.9rem; line-height: 1.6; }
    .insight-action { color: #64748b; font-size: 0.85rem; font-style: italic; margin-top: 0.4rem; }
    
    .keyword-analysis {
        background: #f0f4f8;
        border-radius: 8px;
        padding: 1rem 1.25rem;
        margin-top: 1rem;
    }
    .keyword-analysis-title {
        font-weight: 600;
        color: #1e3a5f;
        font-size: 0.9rem;
        margin-bottom: 0.5rem;
    }
    .keyword-analysis-text {
        color: #475569;
        font-size: 0.85rem;
        line-height: 1.7;
    }
    
    .comment {
        background: #f8fafc;
        padding: 0.9rem 1rem;
        border-radius: 8px;
        margin-bottom: 0.5rem;
        border-left: 3px solid #cbd5e1;
    }
    .comment.pos { border-color: #1e3a5f; }
    .comment.neg { border-color: #94a3b8; }
    .comment-text { color: #334155; font-size: 0.88rem; line-height: 1.5; }
    .comment-likes { color: #94a3b8; font-size: 0.8rem; margin-top: 0.3rem; }
    
    .footer { text-align: center; padding: 2rem 0; color: #94a3b8; font-size: 0.8rem; }
    
    #MainMenu, footer, .stDeployButton {display: none;}
    
    .stButton > button {
        background: #1e3a5f;
        color: white;
        border: none;
        padding: 0.7rem 2rem;
        font-weight: 600;
        border-radius: 8px;
    }
    .stButton > button:hover { background: #2d5a87; }
    
    .stDownloadButton > button {
        background: white;
        color: #1e3a5f;
        border: 2px solid #1e3a5f;
        padding: 0.6rem 1.5rem;
        font-weight: 600;
        border-radius: 8px;
    }
    .stDownloadButton > button:hover {
        background: #1e3a5f;
        color: white;
    }
    
    .stTextInput > div > div > input {
        border-radius: 8px;
        border: 1px solid #e2e8f0;
        padding: 0.7rem 1rem;
    }
    
    [data-testid="stMetricValue"] { font-size: 1.6rem; color: #1e3a5f; }
    [data-testid="stMetricLabel"] { color: #64748b; }
</style>
""", unsafe_allow_html=True)

# =============================================================================
# ì„¤ì •
# =============================================================================
CONFIG = {"max_comments": 800, "top_keywords_count": 12}

STOPWORDS = set(['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ë¡œ', 'ìœ¼ë¡œ',
    'í•˜ê³ ', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ë°', 'ë“±',
    'ë‚˜', 'ë„ˆ', 'ìš°ë¦¬', 'ì €', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê²ƒ', 'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°',
    'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê°™ë‹¤', 'ë³´ë‹¤', 'ì•Œë‹¤', 'ì‹¶ë‹¤', 'ì£¼ë‹¤',
    'í•˜ëŠ”', 'í•˜ë©´', 'í•´ì„œ', 'í–ˆë‹¤', 'í•œë‹¤', 'í• ', 'í•¨', 'ë˜ëŠ”', 'ë˜ë©´', 'ëë‹¤', 'ëœë‹¤',
    'ìˆëŠ”', 'ìˆìœ¼ë©´', 'ìˆê³ ', 'ìˆì–´ì„œ', 'ìˆì—ˆë‹¤', 'ìˆì„', 'ìˆìŒ',
    'ê²ƒ', 'ê±°', 'ìˆ˜', 'ë•Œ', 'ì¤‘', 'ë‚´', 'ë…„', 'ì›”', 'ì¼', 'ë²ˆ', 'ë¶„',
    'ì˜ìƒ', 'ëŒ“ê¸€', 'ë™ì˜ìƒ', 'ìœ íŠœë¸Œ', 'ì±„ë„', 'êµ¬ë…', 'ì¢‹ì•„ìš”', 'ì‹œì²­', 'ì§„ì§œ', 'ë„ˆë¬´', 'ì •ë§',
    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'to', 'of', 'in', 'for', 'on', 'with',
    'i', 'me', 'my', 'you', 'your', 'he', 'she', 'it', 'we', 'they', 'this', 'that', 'and', 'but', 'or',
    'video', 'comment', 'youtube', 'channel', 'subscribe'])

POSITIVE_WORDS = {'ì¢‹ë‹¤', 'ì¢‹ì•„', 'ì¢‹ë„¤', 'ì¢‹ì€', 'ì¢‹ì•˜', 'ì¢‹ìŒ', 'ì¢‹ì•„ìš”', 'ì¢‹ìŠµë‹ˆë‹¤', 'ìµœê³ ', 'ìµœê³ ë‹¤', 'ìµœê³ ì•¼', 'ìµœê³ ì˜ˆìš”', 'ìµœê³ ì„',
    'ëŒ€ë°•', 'ëŒ€ë°•ì´ë‹¤', 'ë©‹ì§€ë‹¤', 'ë©‹ì ¸', 'ë©‹ìˆë‹¤', 'ë©‹ìˆì–´', 'ë©‹ì§', 'ì˜ˆì˜ë‹¤', 'ì˜ˆë»', 'ì˜ˆì¨', 'ì´ì˜ë‹¤', 'ì´ë»',
    'ì‚¬ë‘', 'ì‚¬ë‘í•´', 'ì‚¬ë‘í•´ìš”', 'ì‚¬ë‘í•©ë‹ˆë‹¤', 'ê°ì‚¬', 'ê°ì‚¬í•´ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ê³ ë§ˆì›Œ', 'ê³ ë§™ìŠµë‹ˆë‹¤',
    'í–‰ë³µ', 'í–‰ë³µí•´', 'ê¸°ì˜ë‹¤', 'ì¦ê²ë‹¤', 'ê¸°ëŒ€', 'ê¸°ëŒ€ëœë‹¤', 'ê¸°ëŒ€ë¼', 'ì‘ì›', 'ì‘ì›í•´', 'í™”ì´íŒ…', 'íŒŒì´íŒ…', 'í˜ë‚´',
    'í›Œë¥­', 'ì™„ë²½', 'ê°ë™', 'ì„¤ë ˜', 'ì„¤ë ˆ', 'ì¬ë°Œ', 'ì¬ë°Œë‹¤', 'ì¬ë¯¸ìˆ', 'ì›ƒê¸°ë‹¤', 'ì›ƒê²¨', 'íë§', 'ê·€ì—½', 'ê·€ì—¬ì›Œ',
    'ì˜ìƒ', 'ì˜ìƒê²¼', 'ì¡´ì˜', 'ì¡´ì˜ˆ', 'ì§±', 'ì©”ì–´', 'ì©ë‹¤', 'ë¯¸ì³¤', 'ë¯¸ì³¤ë‹¤', 'ëŒ€ë‹¨', 'ë†€ë', 'ì‹ ê¸°', 'ë ˆì „ë“œ',
    'ì¸ì •', 'ì¶”ì²œ', 'ê°“', 'ì¡´ê²½', 'ì²œì¬', 'ì•„ë¦„ë‹µ', 'í™˜ìƒì ', 'ì—­ì‹œ', 'ë¯¿ê³ ë³´ëŠ”', 'ì°', 'ê¿€ì¼', 'í•µì¼', 'ì¡´ì¼', 'ì†Œë¦„', 'ê°íƒ„', 'ê³µê°',
    'good', 'great', 'best', 'love', 'like', 'amazing', 'awesome', 'beautiful', 'excellent', 'fantastic', 'perfect', 'happy',
    'incredible', 'brilliant', 'wow', 'omg', 'fire', 'goat', 'queen', 'king', 'icon', 'slay', 'legend'}

NEGATIVE_WORDS = {'ì‹«ë‹¤', 'ì‹«ì–´', 'ë³„ë¡œ', 'ìµœì•…', 'ì‹¤ë§', 'ì§œì¦', 'ì§œì¦ë‚˜', 'í™”ë‚˜', 'ë‹µë‹µ', 'ë¶ˆì¾Œ', 'ìŠ¬í”„', 'ìš°ìš¸',
    'ì•„ì‰½', 'ê±±ì •', 'ë¶ˆì•ˆ', 'í˜ë“¤', 'í”¼ê³¤', 'ë‚˜ì˜', 'ëª»í•˜', 'í›„íšŒ', 'í˜ì˜¤', 'ì—­ê²¹', 'ì§€ë£¨', 'ë…¸ì¼', 'ì¬ë¯¸ì—†', 'ë§í–ˆ', 'ë§í•¨', 'ì“°ë ˆê¸°', 'ë¶ˆí¸', 'ë¹„ì¶”',
    'bad', 'worst', 'hate', 'terrible', 'awful', 'sad', 'angry', 'disappointed', 'boring', 'fail', 'trash', 'cringe'}

POSITIVE_EMOJIS = set('ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ¤£ğŸ˜‚ğŸ˜ŠğŸ˜‡ğŸ¥°ğŸ˜ğŸ¤©ğŸ˜˜ğŸ‘ğŸ‘ğŸ™ŒğŸ’ªâœ¨ğŸŒŸâ­ğŸ’–ğŸ’—â¤ğŸ§¡ğŸ’›ğŸ’šğŸ’™ğŸ’œğŸ’ğŸ”¥ğŸ’¯ğŸ‰ğŸ‘‘ğŸ’ğŸ†ğŸ˜ğŸ¤—ğŸ¥³â¤ï¸')
NEGATIVE_EMOJIS = set('ğŸ˜¢ğŸ˜­ğŸ˜¤ğŸ˜ ğŸ˜¡ğŸ¤¬ğŸ’”ğŸ‘ğŸ™„ğŸ˜’ğŸ˜ğŸ˜”ğŸ˜ŸğŸ™ğŸ˜£ğŸ˜–ğŸ˜«ğŸ˜©ğŸ˜±ğŸ¤®ğŸ¤¢')

# =============================================================================
# ìœ í‹¸ë¦¬í‹°
# =============================================================================
def extract_video_id(url):
    if not url: return None
    patterns = [r'(?:youtube\.com/watch\?v=|youtu\.be/|youtube\.com/embed/|youtube\.com/shorts/)([a-zA-Z0-9_-]{11})', r'[?&]v=([a-zA-Z0-9_-]{11})']
    for p in patterns:
        m = re.search(p, url)
        if m: return m.group(1)
    return url if re.match(r'^[a-zA-Z0-9_-]{11}$', url) else None

def clean_text(text):
    if not isinstance(text, str): return ""
    text = re.sub(r'http[s]?://\S+', '', text)
    text = re.sub(r'<[^>]+>', '', text)
    return re.sub(r'\s+', ' ', text).strip()

def preprocess(text):
    text = clean_text(text).lower()
    text = re.compile("[" + u"\U0001F600-\U0001F64F" + u"\U0001F300-\U0001F5FF" + u"\U0001F680-\U0001F6FF" + u"\U0001F1E0-\U0001F1FF" + u"\U00002702-\U000027B0" + "]+", re.UNICODE).sub('', text)
    text = re.sub(r'[^\w\sê°€-í£a-zA-Z0-9]', ' ', text)
    return ' '.join([t for t in text.split() if t not in STOPWORDS and len(t) > 1])

def format_date(d):
    return f"{d[:4]}ë…„ {d[4:6]}ì›” {d[6:8]}ì¼" if d and len(d) == 8 else "ì •ë³´ ì—†ìŒ"

def format_num(n):
    try:
        n = int(n) if n else 0
        if n >= 100000000: return f"{n/100000000:.1f}ì–µ"
        if n >= 10000: return f"{n/10000:.1f}ë§Œ"
        if n >= 1000: return f"{n/1000:.1f}ì²œ"
        return f"{n:,}"
    except: return "0"

# =============================================================================
# ëŒ“ê¸€ ìˆ˜ì§‘
# =============================================================================
@st.cache_data(ttl=1800, show_spinner=False)
def collect_comments(url, max_comments):
    import yt_dlp
    opts = {'quiet': True, 'no_warnings': True, 'extract_flat': False, 'getcomments': True,
            'extractor_args': {'youtube': {'max_comments': [str(max_comments)], 'comment_sort': ['top']}}}
    with yt_dlp.YoutubeDL(opts) as ydl:
        info = ydl.extract_info(url, download=False)
        if not info: return None, []
        video_info = {
            'title': info.get('title', 'ì œëª© ì—†ìŒ'),
            'channel': info.get('channel', info.get('uploader', '')),
            'thumbnail': info.get('thumbnail', ''),
            'view_count': info.get('view_count', 0),
            'like_count': info.get('like_count', 0),
            'upload_date': format_date(info.get('upload_date', '')),
            'url': url,
        }
        raw = info.get('comments') or []
        comments = [{'text': c.get('text', ''), 'like_count': c.get('like_count', 0) or 0} for c in raw[:max_comments] if c]
        return video_info, comments

# =============================================================================
# ê°ì„± ë¶„ì„
# =============================================================================
def analyze_sentiment(text):
    if not text: return 'neutral', 0.0
    text_lower = text.lower()
    score = 0.0
    
    pos_e = sum(1 for e in POSITIVE_EMOJIS if e in text)
    neg_e = sum(1 for e in NEGATIVE_EMOJIS if e in text)
    if pos_e + neg_e > 0: score += (pos_e - neg_e) / (pos_e + neg_e + 1) * 1.5
    
    words = set(re.findall(r'[ê°€-í£]+|[a-z]+', text_lower))
    pos_w = sum(1 for w in words if any(pw in w or w in pw for pw in POSITIVE_WORDS))
    neg_w = sum(1 for w in words if any(nw in w or w in nw for nw in NEGATIVE_WORDS))
    if pos_w + neg_w > 0: score += (pos_w - neg_w) / (pos_w + neg_w + 0.5)
    
    if re.search(r'ã…‹{2,}|ã…{2,}', text): score += 0.3
    if re.search(r'ã…¡ã…¡|;;', text): score -= 0.3
    if text.count('!') >= 2: score += 0.2
    
    if score > 0.1: return 'positive', min(score, 1.0)
    elif score < -0.1: return 'negative', max(score, -1.0)
    return 'neutral', score

# =============================================================================
# í‚¤ì›Œë“œ + ë§ˆì¼€í„° ë¶„ì„
# =============================================================================
def extract_keywords(texts, top_n=12):
    words = []
    for t in texts:
        if t: words.extend(preprocess(str(t)).split())
    return Counter(words).most_common(top_n) if words else []

def generate_keyword_marketing_analysis(keywords, pos_pct, neg_pct, total):
    """10ë…„ì°¨ SNS ë§ˆì¼€í„° ê´€ì ì˜ í‚¤ì›Œë“œ ë¶„ì„"""
    if not keywords:
        return ""
    
    top_kws = [k for k, _ in keywords[:5]]
    kw_str = ', '.join(top_kws)
    top1 = keywords[0][0] if keywords else ""
    top1_count = keywords[0][1] if keywords else 0
    
    analysis = []
    
    # 1. í‚¤ì›Œë“œ ì§‘ì¤‘ë„ ë¶„ì„
    if top1_count > total * 0.1:
        analysis.append(f"â–¸ **í‚¤ì›Œë“œ ì§‘ì¤‘ë„ ë†’ìŒ**: '{top1}'ì´(ê°€) ì „ì²´ ëŒ“ê¸€ì˜ {top1_count/total*100:.1f}%ì—ì„œ ì–¸ê¸‰ë©ë‹ˆë‹¤. "
                       f"ì´ëŠ” ì‹œì²­ìë“¤ì˜ 'í•µì‹¬ ê´€ì‹¬ì‚¬'ê°€ ëª…í™•í•˜ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ì½˜í…ì¸  í¬ì§€ì…”ë‹ì´ ì˜ ë˜ì–´ ìˆê±°ë‚˜, "
                       f"íŠ¹ì • ìš”ì†Œê°€ ê°•ë ¬í•œ ì¸ìƒì„ ë‚¨ê²¼ìŠµë‹ˆë‹¤.")
    else:
        analysis.append(f"â–¸ **í‚¤ì›Œë“œ ë¶„ì‚°í˜•**: íŠ¹ì • í‚¤ì›Œë“œì— ì§‘ì¤‘ë˜ì§€ ì•Šê³  ë‹¤ì–‘í•œ ë°˜ì‘ì´ ë‚˜ì˜µë‹ˆë‹¤. "
                       f"ì‹œì²­ìë§ˆë‹¤ ë‹¤ë¥¸ í¬ì¸íŠ¸ì— ë°˜ì‘í•˜ê³  ìˆì–´, ì½˜í…ì¸ ì˜ 'í•µì‹¬ ë©”ì‹œì§€'ë¥¼ ë” ëª…í™•íˆ í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.")
    
    # 2. ê°ì • vs ë‚´ìš© í‚¤ì›Œë“œ ë¹„ìœ¨
    emotion_words = {'ì‚¬ë‘', 'ê°ë™', 'ëˆˆë¬¼', 'ì†Œë¦„', 'ì„¤ë ˆ', 'í–‰ë³µ', 'ìŠ¬í”„', 'í™”ë‚˜', 'ì§œì¦', 'love', 'happy', 'sad', 'angry'}
    emotion_kws = [k for k, _ in keywords if any(e in k for e in emotion_words)]
    
    if len(emotion_kws) >= 2:
        analysis.append(f"â–¸ **ê°ì • ë°˜ì‘ í™œë°œ**: ê°ì • ê´€ë ¨ í‚¤ì›Œë“œ({', '.join(emotion_kws[:3])})ê°€ ë‹¤ìˆ˜ ë“±ì¥í•©ë‹ˆë‹¤. "
                       f"ì‹œì²­ìë“¤ì´ 'ì •ì„œì ìœ¼ë¡œ ëª°ì…'í•˜ê³  ìˆë‹¤ëŠ” ì‹ í˜¸ì…ë‹ˆë‹¤. ê°ì •ì„ ìê·¹í•˜ëŠ” ì½˜í…ì¸ ëŠ” ê³µìœ ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤.")
    
    # 3. ì½˜í…ì¸  ë°©í–¥ì„± ì œì•ˆ
    analysis.append(f"â–¸ **ì½˜í…ì¸  ë°©í–¥ì„±**: ì‹œì²­ìë“¤ì´ '{top1}'ì— ê°•í•˜ê²Œ ë°˜ì‘í•˜ê³  ìˆìŠµë‹ˆë‹¤. "
                   f"ë‹¤ìŒ ì½˜í…ì¸ ì—ì„œ '{top1}'ë¥¼ ë©”ì¸ í…Œë§ˆë¡œ í™•ì¥í•˜ê±°ë‚˜, "
                   f"'{top1} + {top_kws[1] if len(top_kws) > 1 else 'ê´€ë ¨ì£¼ì œ'}'ë¥¼ ê²°í•©í•œ ê¸°íšì„ ì¶”ì²œí•©ë‹ˆë‹¤.")
    
    # 4. ê´‘ê³ /í˜‘ì°¬ í‚¤ì›Œë“œ ì²´í¬
    ad_words = {'ê´‘ê³ ', 'í˜‘ì°¬', 'ad', 'sponsored', 'ëˆ', 'í™ë³´'}
    ad_found = [k for k, _ in keywords if any(a in k for a in ad_words)]
    if ad_found:
        analysis.append(f"â–¸ **ì£¼ì˜ í•„ìš”**: '{', '.join(ad_found)}' í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. "
                       f"ì‹œì²­ìë“¤ì´ ìƒì—…ì  ìš”ì†Œì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ê³  ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íˆ¬ëª…í•œ ì†Œí†µ ê¶Œì¥.")
    
    # 5. í•´ì‹œíƒœê·¸ ì¶”ì²œ
    hashtags = [f"#{k.replace(' ', '')}" for k, _ in keywords[:5]]
    analysis.append(f"â–¸ **ì¶”ì²œ í•´ì‹œíƒœê·¸**: {' '.join(hashtags)}")
    
    return '\n\n'.join(analysis)

# =============================================================================
# ì°¨íŠ¸
# =============================================================================
def create_donut_chart(pos, neu, neg):
    colors = ['#1e3a5f', '#5a7fa8', '#a8c5de']
    fig = go.Figure(data=[go.Pie(
        values=[pos, neu, neg], labels=['ê¸ì •', 'ì¤‘ë¦½', 'ë¶€ì •'], hole=0.55,
        marker=dict(colors=colors), textinfo='percent', textfont=dict(size=14, color='white'),
        hovertemplate='%{label}: %{value}ê°œ<br>%{percent}<extra></extra>', sort=False
    )])
    fig.update_layout(
        showlegend=True, legend=dict(orientation='h', yanchor='bottom', y=-0.15, xanchor='center', x=0.5, font=dict(size=12)),
        margin=dict(t=20, b=40, l=20, r=20), height=280, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',
    )
    total = pos + neu + neg
    fig.add_annotation(text=f"<b>{total:,}</b><br><span style='font-size:11px;color:#64748b'>ëŒ“ê¸€</span>",
                      x=0.5, y=0.5, font=dict(size=18, color='#1e3a5f'), showarrow=False)
    return fig

def create_keyword_chart(keywords):
    if not keywords: return None
    kw_list = keywords[:10]
    labels = [k for k, _ in kw_list][::-1]
    values = [v for _, v in kw_list][::-1]
    n = len(labels)
    colors = [f'rgba(30, 58, 95, {0.3 + 0.7 * i / (n-1 if n > 1 else 1)})' for i in range(n)]
    
    fig = go.Figure(data=[go.Bar(
        x=values, y=labels, orientation='h', marker=dict(color=colors),
        text=values, textposition='outside', textfont=dict(size=11, color='#1e3a5f'),
        hovertemplate='%{y}: %{x}íšŒ<extra></extra>'
    )])
    fig.update_layout(
        margin=dict(t=20, b=20, l=20, r=40), height=280, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, tickfont=dict(size=12, color='#334155')), bargap=0.3,
    )
    return fig

# =============================================================================
# PDF ìƒì„±
# =============================================================================
def generate_pdf_report(video_info, total, pos, neu, neg, pos_pct, neg_pct, keywords, top_pos_comments, top_neg_comments, marketing_analysis):
    """PDF ë¦¬í¬íŠ¸ ìƒì„±"""
    from reportlab.lib import colors
    from reportlab.lib.pagesizes import A4
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import mm
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont
    import urllib.request
    import os
    
    # í•œê¸€ í°íŠ¸ ì„¤ì • (Noto Sans KR)
    font_path = '/tmp/NotoSansKR-Regular.ttf'
    font_bold_path = '/tmp/NotoSansKR-Bold.ttf'
    
    if not os.path.exists(font_path):
        try:
            urllib.request.urlretrieve(
                'https://github.com/google/fonts/raw/main/ofl/notosanskr/NotoSansKR-Regular.ttf',
                font_path
            )
            urllib.request.urlretrieve(
                'https://github.com/google/fonts/raw/main/ofl/notosanskr/NotoSansKR-Bold.ttf',
                font_bold_path
            )
        except:
            pass
    
    try:
        pdfmetrics.registerFont(TTFont('NotoSansKR', font_path))
        pdfmetrics.registerFont(TTFont('NotoSansKR-Bold', font_bold_path))
        font_name = 'NotoSansKR'
        font_bold = 'NotoSansKR-Bold'
    except:
        font_name = 'Helvetica'
        font_bold = 'Helvetica-Bold'
    
    buffer = io.BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=20*mm, leftMargin=20*mm, topMargin=20*mm, bottomMargin=20*mm)
    
    styles = getSampleStyleSheet()
    styles.add(ParagraphStyle(name='KoreanTitle', fontName=font_bold, fontSize=18, textColor=colors.HexColor('#1e3a5f'), spaceAfter=10))
    styles.add(ParagraphStyle(name='KoreanHeading', fontName=font_bold, fontSize=12, textColor=colors.HexColor('#1e3a5f'), spaceBefore=15, spaceAfter=8))
    styles.add(ParagraphStyle(name='KoreanBody', fontName=font_name, fontSize=10, textColor=colors.HexColor('#334155'), leading=16))
    styles.add(ParagraphStyle(name='KoreanSmall', fontName=font_name, fontSize=9, textColor=colors.HexColor('#64748b'), leading=14))
    
    story = []
    
    # ì œëª©
    story.append(Paragraph("ìœ íŠœë¸Œ ëŒ“ê¸€ ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸", styles['KoreanTitle']))
    story.append(Paragraph(f"ìƒì„±ì¼: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M')}", styles['KoreanSmall']))
    story.append(Spacer(1, 10))
    
    # ì˜ìƒ ì •ë³´
    story.append(Paragraph("ğŸ“º ì˜ìƒ ì •ë³´", styles['KoreanHeading']))
    info_data = [
        ['ì œëª©', video_info.get('title', '')[:50] + ('...' if len(video_info.get('title', '')) > 50 else '')],
        ['ì±„ë„', video_info.get('channel', '')],
        ['ì—…ë¡œë“œ', video_info.get('upload_date', '')],
        ['ì¡°íšŒìˆ˜', format_num(video_info.get('view_count', 0))],
    ]
    info_table = Table(info_data, colWidths=[60, 400])
    info_table.setStyle(TableStyle([
        ('FONTNAME', (0, 0), (-1, -1), font_name),
        ('FONTSIZE', (0, 0), (-1, -1), 10),
        ('TEXTCOLOR', (0, 0), (0, -1), colors.HexColor('#64748b')),
        ('TEXTCOLOR', (1, 0), (1, -1), colors.HexColor('#1e293b')),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
    ]))
    story.append(info_table)
    story.append(Spacer(1, 10))
    
    # ê°ì„± ë¶„ì„
    story.append(Paragraph("ğŸ“Š ê°ì„± ë¶„ì„ ê²°ê³¼", styles['KoreanHeading']))
    sentiment_data = [
        ['ë¶„ì„ ëŒ“ê¸€', f'{total:,}ê°œ', 'ê¸ì •ë¥ ', f'{pos_pct:.1f}%'],
        ['ê¸ì •', f'{pos:,}ê°œ', 'ë¶€ì •', f'{neg:,}ê°œ'],
    ]
    sent_table = Table(sentiment_data, colWidths=[70, 100, 70, 100])
    sent_table.setStyle(TableStyle([
        ('FONTNAME', (0, 0), (-1, -1), font_name),
        ('FONTSIZE', (0, 0), (-1, -1), 10),
        ('TEXTCOLOR', (0, 0), (-1, -1), colors.HexColor('#334155')),
        ('BACKGROUND', (0, 0), (-1, -1), colors.HexColor('#f8fafc')),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 8),
        ('TOPPADDING', (0, 0), (-1, -1), 8),
    ]))
    story.append(sent_table)
    story.append(Spacer(1, 10))
    
    # í‚¤ì›Œë“œ
    story.append(Paragraph("ğŸ”‘ í•µì‹¬ í‚¤ì›Œë“œ", styles['KoreanHeading']))
    if keywords:
        kw_text = ', '.join([f"{k}({v})" for k, v in keywords[:10]])
        story.append(Paragraph(kw_text, styles['KoreanBody']))
    story.append(Spacer(1, 10))
    
    # ë§ˆì¼€íŒ… ë¶„ì„
    story.append(Paragraph("ğŸ’¡ ë§ˆì¼€í„° ê´€ì  ë¶„ì„", styles['KoreanHeading']))
    if marketing_analysis:
        # ë§ˆí¬ë‹¤ìš´ ì œê±°
        clean_analysis = marketing_analysis.replace('**', '').replace('â–¸ ', 'â€¢ ')
        for para in clean_analysis.split('\n\n'):
            if para.strip():
                story.append(Paragraph(para.strip(), styles['KoreanBody']))
                story.append(Spacer(1, 5))
    story.append(Spacer(1, 10))
    
    # ì£¼ìš” ëŒ“ê¸€
    story.append(Paragraph("ğŸ’¬ ì£¼ìš” ê¸ì • ëŒ“ê¸€", styles['KoreanHeading']))
    for c in top_pos_comments[:3]:
        text = c['text'][:100] + ('...' if len(c['text']) > 100 else '')
        story.append(Paragraph(f"â€¢ \"{text}\" (ğŸ‘ {c['like_count']:,})", styles['KoreanSmall']))
    
    story.append(Spacer(1, 10))
    story.append(Paragraph("ğŸ’¬ ì£¼ìš” ë¶€ì • ëŒ“ê¸€", styles['KoreanHeading']))
    for c in top_neg_comments[:3]:
        text = c['text'][:100] + ('...' if len(c['text']) > 100 else '')
        story.append(Paragraph(f"â€¢ \"{text}\" (ğŸ‘ {c['like_count']:,})", styles['KoreanSmall']))
    
    # í‘¸í„°
    story.append(Spacer(1, 20))
    story.append(Paragraph("â”€" * 50, styles['KoreanSmall']))
    story.append(Paragraph("ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ê¸° v7.0 | ìë™ ìƒì„± ë¦¬í¬íŠ¸", styles['KoreanSmall']))
    
    doc.build(story)
    buffer.seek(0)
    return buffer

# =============================================================================
# ë©”ì¸
# =============================================================================
def main():
    # í—¤ë”
    st.markdown('''
    <div class="header">
        <h1>ğŸ“Š ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ê¸°</h1>
        <p>ì˜ìƒ URLì„ ì…ë ¥í•˜ë©´ ëŒ“ê¸€ ì¸ì‚¬ì´íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤</p>
    </div>
    ''', unsafe_allow_html=True)
    
    # ì…ë ¥ (í´ë¦­ ì‹œ ë¦¬í”„ë ˆì‹œ)
    col1, col2, col3 = st.columns([1, 2.5, 1])
    with col2:
        # ì…ë ¥ì°½ í´ë¦­ ì‹œ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ key ì‚¬ìš©
        if st.session_state.clear_input:
            default_value = ""
            st.session_state.clear_input = False
        else:
            default_value = ""
        
        url = st.text_input(
            "URL", 
            value=default_value,
            placeholder="https://www.youtube.com/watch?v=... (í´ë¦­í•˜ë©´ ì´ˆê¸°í™”ë©ë‹ˆë‹¤)", 
            label_visibility="collapsed",
            key="url_input"
        )
        
        col_btn1, col_btn2 = st.columns([2, 1])
        with col_btn1:
            btn = st.button("ğŸ” ë¶„ì„ ì‹œì‘", use_container_width=True)
        with col_btn2:
            if st.button("ğŸ”„ ì´ˆê¸°í™”", use_container_width=True):
                st.session_state.clear_input = True
                st.session_state.analysis_done = False
                st.rerun()
    
    if btn and url:
        vid = extract_video_id(url)
        if not vid:
            st.error("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ URLì…ë‹ˆë‹¤.")
            return
        
        try:
            progress = st.progress(0, "ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...")
            video_info, comments = collect_comments(url, CONFIG["max_comments"])
            progress.progress(50, "ë¶„ì„ ì¤‘...")
            
            if not video_info or not comments:
                st.warning("âš ï¸ ëŒ“ê¸€ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            df = pd.DataFrame(comments)
            results = [analyze_sentiment(str(t)) for t in df['text'].fillna('')]
            df['sentiment'] = [r[0] for r in results]
            
            keywords = extract_keywords(df['text'].tolist(), CONFIG["top_keywords_count"])
            progress.progress(100, "ì™„ë£Œ!")
            progress.empty()
            
            # ê²°ê³¼ ê³„ì‚°
            total = len(df)
            pos = int((df['sentiment'] == 'positive').sum())
            neu = int((df['sentiment'] == 'neutral').sum())
            neg = int((df['sentiment'] == 'negative').sum())
            pos_pct = pos / total * 100 if total else 0
            neg_pct = neg / total * 100 if total else 0
            
            # ë§ˆì¼€íŒ… ë¶„ì„ ìƒì„±
            marketing_analysis = generate_keyword_marketing_analysis(keywords, pos_pct, neg_pct, total)
            
            # ì£¼ìš” ëŒ“ê¸€
            top_pos_df = df[df['sentiment'] == 'positive'].nlargest(3, 'like_count')
            top_neg_df = df[df['sentiment'] == 'negative'].nlargest(3, 'like_count')
            top_pos_comments = top_pos_df.to_dict('records')
            top_neg_comments = top_neg_df.to_dict('records')
            
            # ========== ì˜ìƒ ì •ë³´ ë°•ìŠ¤ ==========
            st.markdown('<div class="video-info-box">', unsafe_allow_html=True)
            c1, c2 = st.columns([1, 2.5])
            with c1:
                if video_info.get('thumbnail'):
                    st.image(video_info['thumbnail'], use_container_width=True)
            with c2:
                st.markdown(f"### {video_info.get('title', '')}")
                st.markdown(f'''
                <div class="video-info-row">
                    <span class="video-info-label">ì±„ë„ëª…</span>
                    <span class="video-info-value">{video_info.get('channel', 'ì •ë³´ ì—†ìŒ')}</span>
                </div>
                <div class="video-info-row">
                    <span class="video-info-label">ì—…ë¡œë“œ ë‚ ì§œ</span>
                    <span class="video-info-value">{video_info.get('upload_date', 'ì •ë³´ ì—†ìŒ')}</span>
                </div>
                <div class="video-info-row">
                    <span class="video-info-label">ì¡°íšŒìˆ˜</span>
                    <span class="video-info-value">{format_num(video_info.get('view_count', 0))}</span>
                </div>
                <div class="video-info-row">
                    <span class="video-info-label">ì¢‹ì•„ìš”</span>
                    <span class="video-info-value">{format_num(video_info.get('like_count', 0))}</span>
                </div>
                ''', unsafe_allow_html=True)
            st.markdown('</div>', unsafe_allow_html=True)
            
            # ì§€í‘œ
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("ë¶„ì„ ëŒ“ê¸€", f"{total:,}ê°œ")
            c2.metric("ê¸ì •ë¥ ", f"{pos_pct:.1f}%")
            c3.metric("ì¤‘ë¦½", f"{neu:,}ê°œ")
            c4.metric("ë¶€ì •ë¥ ", f"{neg_pct:.1f}%")
            
            # PDF ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            try:
                pdf_buffer = generate_pdf_report(
                    video_info, total, pos, neu, neg, pos_pct, neg_pct,
                    keywords, top_pos_comments, top_neg_comments, marketing_analysis
                )
                st.download_button(
                    label="ğŸ“„ PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ",
                    data=pdf_buffer,
                    file_name=f"youtube_analysis_{datetime.now().strftime('%Y%m%d_%H%M')}.pdf",
                    mime="application/pdf"
                )
            except Exception as e:
                st.caption(f"PDF ìƒì„± ë¶ˆê°€: {e}")
            
            # ì°¨íŠ¸
            st.markdown('<div class="section-title">ğŸ“Š ë¶„ì„ ê²°ê³¼</div>', unsafe_allow_html=True)
            c1, c2 = st.columns(2)
            
            with c1:
                st.markdown('<div class="card">', unsafe_allow_html=True)
                st.markdown("**ê°ì„± ë¶„í¬**")
                st.plotly_chart(create_donut_chart(pos, neu, neg), use_container_width=True, config={'displayModeBar': False})
                st.markdown('</div>', unsafe_allow_html=True)
            
            with c2:
                st.markdown('<div class="card">', unsafe_allow_html=True)
                st.markdown("**í•µì‹¬ í‚¤ì›Œë“œ**")
                kw_chart = create_keyword_chart(keywords)
                if kw_chart:
                    st.plotly_chart(kw_chart, use_container_width=True, config={'displayModeBar': False})
                st.markdown('</div>', unsafe_allow_html=True)
            
            # í‚¤ì›Œë“œ ë§ˆì¼€íŒ… ë¶„ì„
            if marketing_analysis:
                st.markdown('<div class="section-title">ğŸ¯ í‚¤ì›Œë“œ ë§ˆì¼€íŒ… ë¶„ì„</div>', unsafe_allow_html=True)
                st.markdown(f'<div class="card"><div class="keyword-analysis-text">{marketing_analysis}</div></div>', unsafe_allow_html=True)
            
            # ì¸ì‚¬ì´íŠ¸
            st.markdown('<div class="section-title">ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸</div>', unsafe_allow_html=True)
            
            top_liked = df.nlargest(min(20, len(df)), 'like_count')
            top_pos_r = (top_liked['sentiment'] == 'positive').sum() / max(len(top_liked), 1) * 100
            
            if pos_pct > 60:
                st.markdown(f'''<div class="insight">
                    <div class="insight-title">ğŸŒŸ ê°•ë ¥í•œ íŒ¬ë¤ ê¸°ë°˜ì˜ ê¸ì •ì  ë°”ì´ëŸ´ ì ì¬ë ¥</div>
                    <div class="insight-desc">ì „ì²´ ëŒ“ê¸€ì˜ <b>{pos_pct:.0f}%</b>ê°€ ê¸ì •ì ì…ë‹ˆë‹¤. ì¢‹ì•„ìš” ìƒìœ„ ëŒ“ê¸€ì˜ <b>{top_pos_r:.0f}%</b>ê°€ ê¸ì •ì¸ ì ì€ ì—¬ë¡  ì£¼ë„ì¸µì´ ìš°í˜¸ì ì´ë¼ëŠ” ì‹ í˜¸ì…ë‹ˆë‹¤.</div>
                    <div class="insight-action">â†’ UGC ìº í˜ì¸, íŒ¬ ì°¸ì—¬í˜• ì±Œë¦°ì§€ ì „ëµ ê¶Œì¥</div>
                </div>''', unsafe_allow_html=True)
            elif pos_pct > 40:
                st.markdown(f'''<div class="insight">
                    <div class="insight-title">ğŸ“ˆ í˜¸ì˜ì ì´ë‚˜ ì—´ì„± íŒ¬ ì „í™˜ í•„ìš”</div>
                    <div class="insight-desc">ê¸ì • ë¹„ìœ¨ <b>{pos_pct:.0f}%</b>ëŠ” ì¢‹ì€ ìˆ˜ì¹˜ì´ë‚˜, ê°€ë²¼ìš´ ê´€ì‹¬ì¸µì´ ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</div>
                    <div class="insight-action">â†’ ë¹„í•˜ì¸ë“œ, íŒ¬ì„œë¹„ìŠ¤ ì½˜í…ì¸ ë¡œ ê´€ê³„ ì‹¬í™” í•„ìš”</div>
                </div>''', unsafe_allow_html=True)
            
            if neg_pct > 20:
                st.markdown(f'''<div class="insight">
                    <div class="insight-title">âš ï¸ ë¶€ì • ì—¬ë¡  íŒŒì•… í•„ìš”</div>
                    <div class="insight-desc">ë¶€ì • ë°˜ì‘ì´ <b>{neg_pct:.0f}%</b>ì…ë‹ˆë‹¤. ì›ì¸ íŒŒì•…ì´ í•„ìš”í•©ë‹ˆë‹¤.</div>
                    <div class="insight-action">â†’ ë¶€ì • ëŒ“ê¸€ ë¶„ì„ í›„ í•´ëª…/ê°œì„  ì˜ì—­ ì‹ë³„</div>
                </div>''', unsafe_allow_html=True)
            
            # ëŒ“ê¸€
            st.markdown('<div class="section-title">ğŸ’¬ ì£¼ìš” ëŒ“ê¸€</div>', unsafe_allow_html=True)
            c1, c2 = st.columns(2)
            
            with c1:
                st.markdown("**ğŸ‘ ê¸ì • TOP 3**")
                for c in top_pos_comments:
                    txt = str(c['text'])[:120] + ('...' if len(str(c['text'])) > 120 else '')
                    st.markdown(f'''<div class="comment pos">
                        <div class="comment-text">"{txt}"</div>
                        <div class="comment-likes">ğŸ‘ {int(c['like_count']):,}</div>
                    </div>''', unsafe_allow_html=True)
            
            with c2:
                st.markdown("**ğŸ‘ ë¶€ì • TOP 3**")
                if top_neg_comments:
                    for c in top_neg_comments:
                        txt = str(c['text'])[:120] + ('...' if len(str(c['text'])) > 120 else '')
                        st.markdown(f'''<div class="comment neg">
                            <div class="comment-text">"{txt}"</div>
                            <div class="comment-likes">ğŸ‘ {int(c['like_count']):,}</div>
                        </div>''', unsafe_allow_html=True)
                else:
                    st.success("ğŸ‰ ë¶€ì • ëŒ“ê¸€ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤!")
            
            st.markdown('<div class="footer">ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ê¸° v7.0</div>', unsafe_allow_html=True)
            
        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜: {str(e)}")

if __name__ == "__main__":
    main()

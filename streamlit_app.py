#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìœ íŠœë¸Œ ëŒ“ê¸€ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ê¸° - Streamlit ë²„ì „
Streamlit Cloud ë¬´ë£Œ ë°°í¬ìš©
"""

import streamlit as st
import pandas as pd
import numpy as np
import re
from typing import List, Dict, Tuple, Optional

# =============================================================================
# í˜ì´ì§€ ì„¤ì •
# =============================================================================

st.set_page_config(
    page_title="ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ê¸°",
    page_icon="ğŸ“Š",
    layout="wide"
)

# =============================================================================
# ì„¤ì •
# =============================================================================

CONFIG = {
    "max_comments": 2000,
    "top_keywords_count": 30,
    "num_topics": 5,
    "min_comment_length": 2,
}

STOPWORDS = set([
    'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ë¡œ', 'ìœ¼ë¡œ',
    'í•˜ê³ ', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ë°', 'ë“±',
    'ë‚˜', 'ë„ˆ', 'ìš°ë¦¬', 'ì €', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê²ƒ', 'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°',
    'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê°™ë‹¤', 'ë³´ë‹¤', 'ì•Œë‹¤', 'ì‹¶ë‹¤', 'ì£¼ë‹¤',
    'í•˜ëŠ”', 'í•˜ë©´', 'í•´ì„œ', 'í–ˆë‹¤', 'í•œë‹¤', 'í• ', 'í•¨', 'ë˜ëŠ”', 'ë˜ë©´', 'ëë‹¤', 'ëœë‹¤',
    'ìˆëŠ”', 'ìˆìœ¼ë©´', 'ìˆê³ ', 'ìˆì–´ì„œ', 'ìˆì—ˆë‹¤', 'ìˆì„', 'ìˆìŒ',
    'ê²ƒ', 'ê±°', 'ìˆ˜', 'ë•Œ', 'ì¤‘', 'ë‚´', 'ë…„', 'ì›”', 'ì¼', 'ë²ˆ', 'ë¶„',
    'ì˜ìƒ', 'ëŒ“ê¸€', 'ë™ì˜ìƒ', 'ìœ íŠœë¸Œ', 'ì±„ë„', 'êµ¬ë…', 'ì¢‹ì•„ìš”', 'ì‹œì²­',
    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',
    'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from', 'as',
    'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he', 'him', 'his', 'she', 'her',
    'it', 'its', 'they', 'them', 'their', 'this', 'that', 'these', 'those',
    'and', 'but', 'if', 'or', 'so', 'than', 'too', 'very', 'just',
    'video', 'comment', 'youtube', 'channel', 'subscribe', 'watch',
])


# =============================================================================
# ìœ í‹¸ë¦¬í‹°
# =============================================================================

def extract_video_id(url: str) -> Optional[str]:
    patterns = [
        r'(?:youtube\.com/watch\?v=|youtu\.be/|youtube\.com/embed/|youtube\.com/shorts/)([a-zA-Z0-9_-]{11})',
        r'[?&]v=([a-zA-Z0-9_-]{11})',
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    if re.match(r'^[a-zA-Z0-9_-]{11}$', url):
        return url
    return None


def clean_text(text: str) -> str:
    if not isinstance(text, str):
        return ""
    text = re.sub(r'http[s]?://\S+', '', text)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def preprocess_for_analysis(text: str) -> str:
    text = clean_text(text)
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"
        u"\U0001F300-\U0001F5FF"
        u"\U0001F680-\U0001F6FF"
        u"\U0001F1E0-\U0001F1FF"
        u"\U00002702-\U000027B0"
        "]+", flags=re.UNICODE)
    text = emoji_pattern.sub('', text)
    text = text.lower()
    text = re.sub(r'[^\w\sê°€-í£a-zA-Z0-9]', ' ', text)
    tokens = text.split()
    tokens = [t for t in tokens if t not in STOPWORDS and len(t) > 1]
    return ' '.join(tokens)


def format_upload_date(date_str: str) -> str:
    if not date_str or len(date_str) != 8:
        return "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
    try:
        return f"{date_str[:4]}.{date_str[4:6]}.{date_str[6:8]}"
    except:
        return date_str


# =============================================================================
# ëŒ“ê¸€ ìˆ˜ì§‘ê¸°
# =============================================================================

@st.cache_data(ttl=3600, show_spinner=False)
def collect_comments(url: str, max_comments: int) -> Tuple[Dict, List[Dict]]:
    """ëŒ“ê¸€ ìˆ˜ì§‘ (ìºì‹± ì ìš©)"""
    import yt_dlp
    
    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'extract_flat': False,
        'getcomments': True,
        'extractor_args': {
            'youtube': {
                'max_comments': [str(max_comments)],
                'comment_sort': ['top'],
            }
        }
    }
    
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=False)
        
        upload_date = info.get('upload_date', '')
        
        video_info = {
            'video_id': info.get('id', ''),
            'title': info.get('title', 'N/A'),
            'channel': info.get('channel', info.get('uploader', 'N/A')),
            'thumbnail': info.get('thumbnail', ''),
            'view_count': info.get('view_count', 0),
            'like_count': info.get('like_count', 0),
            'comment_count': info.get('comment_count', 0),
            'upload_date': format_upload_date(upload_date),
        }
        
        raw_comments = info.get('comments', [])
        comments = []
        for i, c in enumerate(raw_comments):
            if i >= max_comments:
                break
            comments.append({
                'comment_id': c.get('id', str(i)),
                'text': c.get('text', ''),
                'like_count': c.get('like_count', 0),
                'author': c.get('author', ''),
            })
        
        return video_info, comments


# =============================================================================
# ê°ì„± ë¶„ì„ê¸°
# =============================================================================

class SentimentAnalyzer:
    def __init__(self):
        self.positive_words = {
            'ì¢‹ë‹¤', 'ì¢‹ì•„', 'ì¢‹ë„¤', 'ì¢‹ì€', 'ì¢‹ì•˜', 'ì¢‹ìŒ', 'ì¢‹ê³ ', 'ì¢‹ì£ ', 'ì¢‹ì•„ìš”', 'ì¢‹ìŠµë‹ˆë‹¤',
            'ìµœê³ ', 'ìµœê³±ë‹ˆë‹¤', 'ìµœê³ ë‹¤', 'ìµœê³ ì•¼', 'ìµœê³ ì˜ˆìš”', 'ìµœê³ ì„', 'ìµœê³ ì—ìš”',
            'ëŒ€ë°•', 'ëŒ€ë°•ì´ë‹¤', 'ëŒ€ë°•ì´ì•¼', 'ëŒ€ë°•ì´ë„¤',
            'ë©‹ì§€ë‹¤', 'ë©‹ì ¸', 'ë©‹ìˆë‹¤', 'ë©‹ìˆì–´', 'ë©‹ì§', 'ë©‹ì§„',
            'ì˜ˆì˜ë‹¤', 'ì˜ˆë»', 'ì˜ˆì¨', 'ì˜ˆìœ', 'ì´ì˜ë‹¤', 'ì´ë»', 'ì´ì¨',
            'ì‚¬ë‘', 'ì‚¬ë‘í•´', 'ì‚¬ë‘í•´ìš”', 'ì‚¬ë‘í•©ë‹ˆë‹¤', 'ì‚¬ë‘ìŠ¤ëŸ¬', 'ì‚¬ë‘ìŠ¤ëŸ½',
            'ê°ì‚¬', 'ê°ì‚¬í•´ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ê³ ë§ˆì›Œ', 'ê³ ë§™ìŠµë‹ˆë‹¤', 'ê³ ë§ˆì›Œìš”',
            'í–‰ë³µ', 'í–‰ë³µí•´', 'í–‰ë³µí•˜ë‹¤', 'ê¸°ì˜ë‹¤', 'ê¸°ë»', 'ì¦ê²ë‹¤', 'ì¦ê±°ì›Œ',
            'ê¸°ëŒ€', 'ê¸°ëŒ€ëœë‹¤', 'ê¸°ëŒ€ë¼', 'ê¸°ëŒ€ë©ë‹ˆë‹¤', 'ê¸°ëŒ€í•´',
            'ì‘ì›', 'ì‘ì›í•´', 'ì‘ì›í•©ë‹ˆë‹¤', 'ì‘ì›í•´ìš”', 'í™”ì´íŒ…', 'íŒŒì´íŒ…', 'í˜ë‚´',
            'ì¶•í•˜', 'ì¶•í•˜í•´', 'ì¶•í•˜í•©ë‹ˆë‹¤', 'ì¶•í•˜ë“œë ¤ìš”',
            'í›Œë¥­', 'í›Œë¥­í•´', 'í›Œë¥­í•˜ë‹¤', 'ì™„ë²½', 'ì™„ë²½í•´', 'ì™„ë²½í•˜ë‹¤',
            'ê°ë™', 'ê°ë™ì´ë‹¤', 'ê°ë™ì´ì•¼', 'ê°ë™ë°›ì•˜', 'ê°ë™ì ',
            'ì¬ë°Œ', 'ì¬ë°Œë‹¤', 'ì¬ë°Œì–´', 'ì¬ë°Œë„¤', 'ì¬ë¯¸ìˆ', 'ì¬ë¯¸ìˆë‹¤', 'ì¬ë¯¸ìˆì–´',
            'ì›ƒê¸°ë‹¤', 'ì›ƒê²¨', 'ì›ƒê¹€', 'ì›ƒê¸´',
            'íë§', 'íë§ëœë‹¤', 'íë§ì´ë‹¤',
            'ê·€ì—½', 'ê·€ì—¬ì›Œ', 'ê·€ì—½ë‹¤', 'ê·€ì—¬ìš´', 'ê¹œì°',
            'ì˜ìƒ', 'ì˜ìƒê²¼', 'ì˜ìƒê¹€', 'ì¡´ì˜', 'í•¸ì„¬',
            'ì¡´ì˜ˆ', 'ê°œì˜ˆ', 'ê²ë‚˜ì˜ˆë»', 'ë„ˆë¬´ì˜ˆë»',
            'ì§±', 'ì§±ì´ë‹¤', 'ì§±ì´ì•¼', 'ì©”ì–´', 'ì©ë‹¤', 'ì©”ì—ˆ', 'ì©”ì–´ìš”',
            'ë¯¸ì³¤', 'ë¯¸ì³¤ë‹¤', 'ë¯¸ì³¤ì–´', 'ë¯¸ì¹¨', 'ë¯¸ì¹œ',
            'ì£½ëŠ”ë‹¤', 'ì£½ê² ë‹¤', 'ì£½ì„ê²ƒê°™', 'ì£½ìŒ',
            'ëŒ€ë‹¨', 'ëŒ€ë‹¨í•´', 'ëŒ€ë‹¨í•˜ë‹¤', 'ëŒ€ë‹¨í•˜ë„¤',
            'ë†€ë', 'ë†€ë¼ì›Œ', 'ë†€ëë‹¤', 'ì‹ ê¸°', 'ì‹ ê¸°í•´', 'ì‹ ê¸°í•˜ë‹¤',
            'ë ˆì „ë“œ', 'ë ˆì „ë”ë¦¬', 'legend', 'goat',
            'ì¸ì •', 'ì¸ì •í•©ë‹ˆë‹¤', 'ì¸ì •ì´ìš”',
            'ì¶”ì²œ', 'ì¶”ì²œí•´', 'ì¶”ì²œí•©ë‹ˆë‹¤',
            'ê°“', 'ê°“ë²½', 'god',
            'ì¡´ê²½', 'ì¡´ê²½í•´', 'ì¡´ê²½í•©ë‹ˆë‹¤', 'ë¦¬ìŠ¤í™', 'respect',
            'ë©‹ìˆ', 'ê°„ì§€', 'ê°„ì§€ë‚˜', 'ê°„ì§€ë‚¨', 'í¼', 'í¼ë‚˜',
            'ì²œì¬', 'ì²œì¬ë‹¤', 'ì²œì¬ì ',
            'ì•„ë¦„ë‹µ', 'ì•„ë¦„ë‹¤ì›Œ', 'ì•„ë¦„ë‹¤ìš´', 'í™©í™€', 'í™˜ìƒì ',
            'ì—­ì‹œ', 'ì—­ì‹œë‚˜', 'ë¯¿ê³ ë³´ëŠ”', 'ë¯¿ë³´',
            'ì°', 'ì°ì´ë‹¤', 'ì°ì´ì•¼', 'ë¦¬ì–¼', 'ì§„ì§œë‹¤',
            'ê¿€ì¼', 'í•µì¼', 'ì¡´ì¼', 'ê°œê¿€', 'ê°œì´ë“',
            'ì†Œë¦„', 'ì†Œë¦„ë‹', 'ì „ìœ¨', 'ê°íƒ„',
            'ëˆˆë¬¼', 'ëˆˆë¬¼ë‚˜', 'ìš¸ì»¥', 'ì°¡', 'ì°¡í•˜ë‹¤',
            'í˜ì´ëœë‹¤', 'ìœ„ë¡œê°€ëœë‹¤', 'ê³µê°', 'ê³µê°ë¼',
            'good', 'great', 'best', 'love', 'like', 'amazing', 'awesome', 'wonderful',
            'beautiful', 'excellent', 'fantastic', 'nice', 'perfect', 'happy', 'cool',
            'incredible', 'brilliant', 'outstanding', 'superb', 'magnificent', 'lovely',
            'talented', 'genius', 'wow', 'omg', 'fire', 'lit', 'sick', 'dope',
            'queen', 'king', 'icon', 'iconic', 'slay', 'slayed', 'serve', 'ate',
            'proud', 'blessed', 'grateful', 'touched', 'moved',
            'support', 'stan', 'bias', 'fave', 'favorite',
        }
        
        self.negative_words = {
            'ì‹«ë‹¤', 'ì‹«ì–´', 'ì‹«ìŒ', 'ì‹«ë„¤',
            'ë³„ë¡œ', 'ë³„ë¡œë‹¤', 'ë³„ë¡œì•¼', 'ë³„ë£¨',
            'ìµœì•…', 'ìµœì•…ì´ë‹¤', 'ìµœì•…ì´ì•¼',
            'ì‹¤ë§', 'ì‹¤ë§ì´ë‹¤', 'ì‹¤ë§ì´ì•¼', 'ì‹¤ë§í–ˆ', 'ì‹¤ë§ìŠ¤ëŸ½',
            'ì§œì¦', 'ì§œì¦ë‚˜', 'ì§œì¦ë‚œë‹¤', 'ì§œì¦ë‚˜ë„¤',
            'í™”ë‚˜', 'í™”ë‚œë‹¤', 'í™”ë‚¨', 'ë¶„ë…¸', 'ì—´ë°›',
            'ë‹µë‹µ', 'ë‹µë‹µí•˜ë‹¤', 'ë‹µë‹µí•´',
            'ë¶ˆì¾Œ', 'ë¶ˆì¾Œí•˜ë‹¤', 'ë¶ˆì¾Œí•´',
            'ìŠ¬í”„', 'ìŠ¬í¼', 'ìŠ¬í”„ë‹¤', 'ìŠ¬í””', 'ìš°ìš¸', 'ìš°ìš¸í•˜ë‹¤',
            'ì•ˆíƒ€ê¹', 'ì•ˆíƒ€ê¹Œì›Œ', 'ì•„ì‰½', 'ì•„ì‰½ë‹¤', 'ì•„ì‰¬ì›Œ',
            'ê±±ì •', 'ê±±ì •ëœë‹¤', 'ê±±ì •ë¼', 'ë¶ˆì•ˆ', 'ë¶ˆì•ˆí•˜ë‹¤',
            'í˜ë“¤', 'í˜ë“¤ë‹¤', 'í˜ë“¤ì–´', 'ì§€ì¹¨', 'ì§€ì³¤', 'í”¼ê³¤',
            'ë‚˜ì˜', 'ë‚˜ë¹ ', 'ë‚˜ìœ', 'ëª»í•˜', 'ëª»í•¨', 'ëª»í•´',
            'í›„íšŒ', 'í›„íšŒëœë‹¤', 'í›„íšŒë¼',
            'í˜ì˜¤', 'ì—­ê²¹', 'ì—­ê²¨ì›Œ', 'êµ¬ì—­ì§ˆ',
            'ì§€ë£¨', 'ì§€ë£¨í•˜ë‹¤', 'ì§€ë£¨í•´', 'ë…¸ì¼', 'ì¬ë¯¸ì—†',
            'ë§í–ˆ', 'ë§í•¨', 'ë§ì‘', 'í­ë§', 'ì«„ë”±ë§',
            'ì“°ë ˆê¸°', 'ì“°ë ‰', 'ë³„ì í…ŒëŸ¬',
            'ê±°ë¶€ê°', 'ë¶ˆí¸', 'ë¶ˆí¸í•˜ë‹¤',
            'ë¹„ì¶”', 'ë¹„ì¶”ì²œ', 'ë¹„ì¶”ë‹¤',
            'bad', 'worst', 'hate', 'dislike', 'terrible', 'awful', 'horrible',
            'sad', 'angry', 'disappointed', 'disappointing', 'boring', 'annoying',
            'frustrated', 'worried', 'fail', 'failed', 'failure', 'wrong',
            'disgusting', 'pathetic', 'waste', 'garbage', 'trash', 'cringe',
            'overrated', 'underwhelming', 'meh',
        }
        
        self.positive_emojis = {
            'ğŸ˜€', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜', 'ğŸ˜†', 'ğŸ˜…', 'ğŸ¤£', 'ğŸ˜‚', 'ğŸ™‚', 'ğŸ˜Š', 'ğŸ˜‡',
            'ğŸ¥°', 'ğŸ˜', 'ğŸ¤©', 'ğŸ˜˜', 'ğŸ˜—', 'ğŸ˜š', 'ğŸ˜™', 'ğŸ¥²',
            'ğŸ‘', 'ğŸ‘', 'ğŸ™Œ', 'ğŸ’ª', 'âœ¨', 'ğŸŒŸ', 'â­', 'ğŸ’–', 'ğŸ’—', 'ğŸ’“', 'ğŸ’•',
            'â¤ï¸', 'ğŸ§¡', 'ğŸ’›', 'ğŸ’š', 'ğŸ’™', 'ğŸ’œ', 'ğŸ–¤', 'ğŸ¤', 'ğŸ’', 'ğŸ’˜', 'â™¥ï¸', 'â¤',
            'ğŸ”¥', 'ğŸ’¯', 'ğŸ‰', 'ğŸŠ', 'ğŸ‘‘', 'ğŸ’', 'ğŸ†', 'ğŸ¥‡',
            'ğŸ˜', 'ğŸ¤—', 'ğŸ¥³', 'ğŸ˜‹', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ¤­', 'ğŸ«¶', 'ğŸ«°',
            'ğŸ‘€', 'ğŸ’€',
        }
        
        self.negative_emojis = {
            'ğŸ˜¢', 'ğŸ˜­', 'ğŸ˜¤', 'ğŸ˜ ', 'ğŸ˜¡', 'ğŸ¤¬', 'ğŸ˜ˆ', 'ğŸ‘¿',
            'ğŸ’”', 'ğŸ‘', 'ğŸ™„', 'ğŸ˜’', 'ğŸ˜', 'ğŸ˜”', 'ğŸ˜Ÿ', 'ğŸ˜•', 'ğŸ™', 'â˜¹ï¸',
            'ğŸ˜£', 'ğŸ˜–', 'ğŸ˜«', 'ğŸ˜©', 'ğŸ¥º', 'ğŸ˜°', 'ğŸ˜¨', 'ğŸ˜±', 'ğŸ¤®', 'ğŸ¤¢',
        }
        
        self.positive_patterns = [
            r'ã…‹{2,}', r'ã…{2,}', r'ï¼{2,}|!{2,}', r'â™¡+|â™¥+',
            r'ìµœê³ +', r'ëŒ€ë°•+', r'ë¯¸ì³¤+', r'í—+', r'ì™€+[ã…-ã…£]*', r'ìš°+ì™€+',
        ]
        
        self.negative_patterns = [
            r'ã…¡ã…¡+', r';;+', r'ì—íœ´+|ì—í˜€+', r'í•œìˆ¨',
        ]
    
    def analyze(self, text: str) -> Tuple[str, float]:
        if not text or not isinstance(text, str):
            return 'neutral', 0.0
        
        text_lower = text.lower()
        score = 0.0
        
        # ì´ëª¨ì§€
        pos_emoji = sum(1 for e in self.positive_emojis if e in text)
        neg_emoji = sum(1 for e in self.negative_emojis if e in text)
        if pos_emoji + neg_emoji > 0:
            score += (pos_emoji - neg_emoji) / (pos_emoji + neg_emoji + 1) * 1.5
        
        # ë‹¨ì–´
        words = set(re.findall(r'[ê°€-í£]+|[a-z]+', text_lower))
        pos_count = sum(1 for w in words if any(pw in w or w in pw for pw in self.positive_words))
        neg_count = sum(1 for w in words if any(nw in w or w in nw for nw in self.negative_words))
        if pos_count + neg_count > 0:
            score += (pos_count - neg_count) / (pos_count + neg_count + 0.5)
        
        # íŒ¨í„´
        pos_pat = sum(1 for p in self.positive_patterns if re.search(p, text))
        neg_pat = sum(1 for p in self.negative_patterns if re.search(p, text))
        score += (pos_pat - neg_pat) * 0.24
        
        # ëŠë‚Œí‘œ
        exclaim = text.count('!') + text.count('ï¼')
        if exclaim >= 2:
            score += 0.15
        
        if score > 0.1:
            return 'positive', min(score, 1.0)
        elif score < -0.1:
            return 'negative', max(score, -1.0)
        return 'neutral', score
    
    def analyze_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        results = [self.analyze(text) for text in df['text'].fillna('')]
        df['sentiment_label'] = [r[0] for r in results]
        df['sentiment_score'] = [r[1] for r in results]
        return df


# =============================================================================
# í‚¤ì›Œë“œ ë¶„ì„
# =============================================================================

def extract_keywords(texts: List[str], top_n: int = 20) -> List[Tuple[str, float]]:
    from sklearn.feature_extraction.text import TfidfVectorizer
    
    processed = [preprocess_for_analysis(t) for t in texts if preprocess_for_analysis(t)]
    if len(processed) < 10:
        return []
    
    try:
        vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=2000, min_df=2, max_df=0.85)
        tfidf = vectorizer.fit_transform(processed)
        features = vectorizer.get_feature_names_out()
        mean_tfidf = np.asarray(tfidf.mean(axis=0)).flatten()
        top_idx = mean_tfidf.argsort()[::-1][:top_n]
        return [(features[i], mean_tfidf[i]) for i in top_idx]
    except:
        return []


# =============================================================================
# í† í”½ ëª¨ë¸ë§
# =============================================================================

def perform_topic_modeling(texts: List[str], num_topics: int = 5) -> Dict:
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.decomposition import LatentDirichletAllocation
    
    processed = [preprocess_for_analysis(t) for t in texts if preprocess_for_analysis(t)]
    if len(processed) < num_topics * 10:
        return {}
    
    try:
        vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=1500, min_df=3, max_df=0.8)
        doc_term = vectorizer.fit_transform(processed)
        lda = LatentDirichletAllocation(n_components=num_topics, max_iter=10, random_state=42)
        doc_topics = lda.fit_transform(doc_term)
        features = vectorizer.get_feature_names_out()
        
        topics = {}
        for idx, topic in enumerate(lda.components_):
            top_idx = topic.argsort()[:-11:-1]
            topics[idx] = {'keywords': [features[i] for i in top_idx]}
        
        return {'topics': topics, 'assignments': doc_topics.argmax(axis=1)}
    except:
        return {}


def get_topic_summaries(df: pd.DataFrame, topic_result: Dict, num_topics: int = 5) -> List[Dict]:
    if not topic_result:
        return []
    
    assignments = topic_result['assignments']
    valid_mask = df['text'].apply(lambda x: len(preprocess_for_analysis(str(x))) > 0)
    valid_df = df[valid_mask].reset_index(drop=True)
    
    min_len = min(len(valid_df), len(assignments))
    valid_df = valid_df.iloc[:min_len].copy()
    valid_df['topic'] = assignments[:min_len]
    
    summaries = []
    for idx in range(num_topics):
        topic_df = valid_df[valid_df['topic'] == idx]
        if len(topic_df) == 0:
            continue
        
        sent_dist = topic_df['sentiment_label'].value_counts(normalize=True)
        sentiment = {
            'positive': sent_dist.get('positive', 0) * 100,
            'neutral': sent_dist.get('neutral', 0) * 100,
            'negative': sent_dist.get('negative', 0) * 100,
        }
        
        summaries.append({
            'keywords': topic_result['topics'][idx]['keywords'][:6],
            'count': len(topic_df),
            'pct': len(topic_df) / len(valid_df) * 100,
            'sentiment': sentiment,
        })
    
    summaries.sort(key=lambda x: x['count'], reverse=True)
    return summaries


# =============================================================================
# ì¸ì‚¬ì´íŠ¸ ìƒì„±
# =============================================================================

def generate_insights(pos_pct: float, neg_pct: float, top_pos_ratio: float, 
                      keywords: List, topic_summaries: List) -> List[Dict]:
    insights = []
    
    if pos_pct > 60:
        insights.append({
            'title': 'ğŸ”¥ ê°•ë ¥í•œ íŒ¬ë¤ ê¸°ë°˜ì˜ ê¸ì •ì  ë°”ì´ëŸ´ ì ì¬ë ¥',
            'desc': f'ì „ì²´ ëŒ“ê¸€ì˜ {pos_pct:.0f}%ê°€ ê¸ì •ì  ë°˜ì‘ì…ë‹ˆë‹¤. ì´ëŠ” "ìë°œì  í™ë³´ ì˜ì§€"ë¥¼ ê°€ì§„ íŒ¬ì¸µì´ í˜•ì„±ë˜ì–´ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¢‹ì•„ìš” ìƒìœ„ ëŒ“ê¸€ì˜ {top_pos_ratio:.0f}%ê°€ ê¸ì •ì¸ ì ì€ ì—¬ë¡  ì£¼ë„ì¸µì´ ìš°í˜¸ì ì´ë¼ëŠ” ì‹ í˜¸ì…ë‹ˆë‹¤.',
            'action': 'UGC ìº í˜ì¸, íŒ¬ ì°¸ì—¬í˜• ì±Œë¦°ì§€ ë“± "íŒ¬ì´ í™ë³´ëŒ€ì‚¬ê°€ ë˜ëŠ”" ì „ëµ ì¶”ì²œ'
        })
    elif pos_pct > 40:
        insights.append({
            'title': 'ğŸ‘€ í˜¸ì˜ì ì´ë‚˜ "ì—´ì„± íŒ¬"ìœ¼ë¡œ ì „í™˜ë˜ì§€ ì•Šì€ ì¸µ ì¡´ì¬',
            'desc': f'ê¸ì • ë¹„ìœ¨ {pos_pct:.0f}%ëŠ” ê´œì°®ì€ ìˆ˜ì¹˜ì´ë‚˜, "ì¢‹ì•„í•˜ì§€ë§Œ êµ³ì´ ì°¾ì•„ë³´ì§„ ì•ŠëŠ”" ê°€ë²¼ìš´ ê´€ì‹¬ì¸µì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.',
            'action': 'ì •ê¸°ì  í„°ì¹˜í¬ì¸íŠ¸(ë¹„í•˜ì¸ë“œ, íŒ¬ì„œë¹„ìŠ¤ ì½˜í…ì¸ )ë¡œ ê´€ê³„ ê¹Šì´ë¥¼ ë”í•´ì•¼ í•¨'
        })
    
    if neg_pct > 20:
        insights.append({
            'title': 'âš ï¸ ë¶€ì • ì—¬ë¡ ì˜ "í•µì‹¬ ë¶ˆë§Œ" íŒŒì•… í•„ìš”',
            'desc': f'ë¶€ì • ë°˜ì‘ì´ {neg_pct:.0f}%ë¡œ ë¬´ì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì½˜í…ì¸  í€„ë¦¬í‹° ë¬¸ì œì¸ì§€, ê¸°ëŒ€ì™€ì˜ ê´´ë¦¬ì¸ì§€ íŒŒì•…ì´ í•„ìš”í•©ë‹ˆë‹¤.',
            'action': 'ë¶€ì • ëŒ“ê¸€ í‚¤ì›Œë“œ ë¶„ì„ í›„, í•´ëª…/ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­ ì‹ë³„'
        })
    
    if topic_summaries:
        t = topic_summaries[0]
        insights.append({
            'title': f'ğŸ’¬ "{t["keywords"][0]}" - ì‹œì²­ìê°€ ê°€ì¥ ë§í•˜ê³  ì‹¶ì–´í•˜ëŠ” ì£¼ì œ',
            'desc': f'ì „ì²´ ëŒ“ê¸€ì˜ {t["pct"]:.0f}%ê°€ ì´ ì£¼ì œë¥¼ ì–¸ê¸‰í•©ë‹ˆë‹¤. ì‚¬ëŒë“¤ì€ ê´€ì‹¬ ì—†ëŠ” ê²ƒì— ëŒ“ê¸€ì„ ë‹¬ì§€ ì•ŠìŠµë‹ˆë‹¤.',
            'action': f'"{t["keywords"][0]}" ì£¼ì œë¡œ í›„ì† ì½˜í…ì¸  ê¸°íš ì‹œ ë†’ì€ engagement ì˜ˆìƒ'
        })
    
    if keywords:
        kws = [k for k, _ in keywords[:5]]
        insights.append({
            'title': 'ğŸ“ ì‹œì²­ì ì–¸ì–´ë¡œ ë§í•˜ë¼',
            'desc': f'ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ: "{", ".join(kws)}". ë§ˆì¼€íŒ… ë©”ì‹œì§€, ì¸ë„¤ì¼, ì œëª©ì— í™œìš©í•˜ë©´ ê³µê°ì„ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
            'action': f'ë‹¤ìŒ ì½˜í…ì¸ ì— "{kws[0]}", "{kws[1] if len(kws) > 1 else ""}" ì „ëµì  í™œìš©'
        })
    
    return insights


# =============================================================================
# ë©”ì¸ ì•±
# =============================================================================

def main():
    # CSS
    st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 700;
        color: #1e3a5f;
        text-align: center;
        margin-bottom: 0.5rem;
    }
    .sub-header {
        font-size: 1rem;
        color: #64748b;
        text-align: center;
        margin-bottom: 2rem;
    }
    .insight-box {
        background: #f8fafc;
        border-left: 4px solid #1e3a5f;
        padding: 1rem 1.5rem;
        margin-bottom: 1rem;
        border-radius: 0 8px 8px 0;
    }
    .insight-title {
        font-weight: 600;
        color: #1e3a5f;
        margin-bottom: 0.5rem;
    }
    .insight-action {
        font-size: 0.9rem;
        color: #2d5a87;
        font-style: italic;
        margin-top: 0.5rem;
    }
    .topic-tag {
        display: inline-block;
        background: #d4e4f1;
        color: #1e3a5f;
        padding: 0.2rem 0.8rem;
        border-radius: 12px;
        font-size: 0.85rem;
        margin: 0.2rem;
    }
    .comment-box {
        background: #f8fafc;
        padding: 1rem;
        border-radius: 8px;
        margin-bottom: 0.5rem;
        border-left: 3px solid #7ba3cc;
    }
    .comment-box.positive { border-color: #2d5a87; }
    .comment-box.negative { border-color: #8b4557; }
    </style>
    """, unsafe_allow_html=True)
    
    # í—¤ë”
    st.markdown('<p class="main-header">ğŸ“Š ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ê¸°</p>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">ì˜ìƒ URLì„ ì…ë ¥í•˜ë©´ ëŒ“ê¸€ ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤</p>', unsafe_allow_html=True)
    
    # URL ì…ë ¥
    col1, col2 = st.columns([4, 1])
    with col1:
        url = st.text_input("ìœ íŠœë¸Œ URL", placeholder="https://www.youtube.com/watch?v=...", label_visibility="collapsed")
    with col2:
        analyze_btn = st.button("ğŸ” ë¶„ì„", type="primary", use_container_width=True)
    
    if analyze_btn and url:
        video_id = extract_video_id(url)
        if not video_id:
            st.error("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ YouTube URLì…ë‹ˆë‹¤.")
            return
        
        try:
            # ì§„í–‰ ìƒíƒœ
            progress = st.progress(0)
            status = st.empty()
            
            status.text("ğŸ“¥ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘... (1~2ë¶„ ì†Œìš”)")
            progress.progress(10)
            
            video_info, comments = collect_comments(url, CONFIG["max_comments"])
            
            if not comments:
                st.error("âŒ ëŒ“ê¸€ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            progress.progress(40)
            status.text("ğŸ” ê°ì„± ë¶„ì„ ì¤‘...")
            
            df = pd.DataFrame(comments)
            analyzer = SentimentAnalyzer()
            df = analyzer.analyze_dataframe(df)
            
            progress.progress(60)
            status.text("ğŸ“Š í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
            
            texts = df[df['text'].str.len() >= CONFIG["min_comment_length"]]['text'].tolist()
            keywords = extract_keywords(texts, CONFIG["top_keywords_count"])
            
            progress.progress(80)
            status.text("ğŸ·ï¸ í† í”½ ë¶„ì„ ì¤‘...")
            
            topic_result = perform_topic_modeling(texts, CONFIG["num_topics"])
            topic_summaries = get_topic_summaries(df, topic_result, CONFIG["num_topics"])
            
            progress.progress(100)
            status.empty()
            progress.empty()
            
            # =====================
            # ê²°ê³¼ í‘œì‹œ
            # =====================
            
            st.divider()
            
            # ì˜ìƒ ì •ë³´
            col1, col2 = st.columns([1, 3])
            with col1:
                if video_info.get('thumbnail'):
                    st.image(video_info['thumbnail'], use_container_width=True)
            with col2:
                st.markdown(f"### {video_info.get('title', 'N/A')}")
                st.markdown(f"**{video_info.get('channel', '')}** Â· ì—…ë¡œë“œ: {video_info.get('upload_date', 'N/A')}")
            
            st.divider()
            
            # ì£¼ìš” ì§€í‘œ
            total = len(df)
            pos_count = (df['sentiment_label'] == 'positive').sum()
            neu_count = (df['sentiment_label'] == 'neutral').sum()
            neg_count = (df['sentiment_label'] == 'negative').sum()
            pos_pct = pos_count / total * 100 if total > 0 else 0
            neg_pct = neg_count / total * 100 if total > 0 else 0
            
            top_liked = df.nlargest(20, 'like_count')
            top_pos_ratio = (top_liked['sentiment_label'] == 'positive').sum() / len(top_liked) * 100
            
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("ë¶„ì„ ëŒ“ê¸€", f"{total:,}ê°œ")
            col2.metric("ê¸ì •ë¥ ", f"{pos_pct:.1f}%")
            col3.metric("ë¶€ì •ë¥ ", f"{neg_pct:.1f}%")
            col4.metric("ì¡°íšŒìˆ˜", f"{video_info.get('view_count', 0):,}")
            
            st.divider()
            
            # ì°¨íŠ¸
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### ê°ì„± ë¶„í¬")
                import plotly.express as px
                fig = px.pie(
                    values=[pos_count, neu_count, neg_count],
                    names=['ê¸ì •', 'ì¤‘ë¦½', 'ë¶€ì •'],
                    color_discrete_sequence=['#2d5a87', '#a8c5de', '#8b4557'],
                    hole=0.4
                )
                fig.update_traces(textposition='inside', textinfo='percent+label')
                fig.update_layout(showlegend=False, margin=dict(t=20, b=20, l=20, r=20))
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.markdown("#### í•µì‹¬ í‚¤ì›Œë“œ")
                if keywords:
                    kw_df = pd.DataFrame(keywords[:10], columns=['í‚¤ì›Œë“œ', 'ì ìˆ˜'])
                    kw_df['ì ìˆ˜'] = kw_df['ì ìˆ˜'] * 1000
                    fig = px.bar(
                        kw_df, y='í‚¤ì›Œë“œ', x='ì ìˆ˜', orientation='h',
                        color_discrete_sequence=['#2d5a87']
                    )
                    fig.update_layout(showlegend=False, margin=dict(t=20, b=20, l=20, r=20))
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("í‚¤ì›Œë“œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            st.divider()
            
            # ì¸ì‚¬ì´íŠ¸
            st.markdown("### ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸")
            insights = generate_insights(pos_pct, neg_pct, top_pos_ratio, keywords, topic_summaries)
            for ins in insights:
                st.markdown(f"""
                <div class="insight-box">
                    <div class="insight-title">{ins['title']}</div>
                    <div>{ins['desc']}</div>
                    <div class="insight-action">â†’ {ins['action']}</div>
                </div>
                """, unsafe_allow_html=True)
            
            st.divider()
            
            # í† í”½ & ëŒ“ê¸€
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### ğŸ·ï¸ í† í”½ë³„ ë¶„ì„")
                for i, t in enumerate(topic_summaries[:5], 1):
                    with st.expander(f"í† í”½ {i}: {', '.join(t['keywords'][:3])} ({t['count']:,}ê°œ, {t['pct']:.0f}%)"):
                        tags = ''.join([f'<span class="topic-tag">{k}</span>' for k in t['keywords'][:6]])
                        st.markdown(tags, unsafe_allow_html=True)
                        st.progress(t['sentiment']['positive'] / 100)
                        st.caption(f"ê¸ì • {t['sentiment']['positive']:.0f}% / ì¤‘ë¦½ {t['sentiment']['neutral']:.0f}% / ë¶€ì • {t['sentiment']['negative']:.0f}%")
            
            with col2:
                st.markdown("#### ğŸ’¬ ì£¼ìš” ëŒ“ê¸€")
                
                st.markdown("**ê¸ì • ë°˜ì‘**")
                for _, row in df[df['sentiment_label'] == 'positive'].nlargest(3, 'like_count').iterrows():
                    text = str(row['text'])[:100] + ('...' if len(str(row['text'])) > 100 else '')
                    st.markdown(f"""
                    <div class="comment-box positive">
                        "{text}"<br>
                        <small>ğŸ‘ {row['like_count']:,}</small>
                    </div>
                    """, unsafe_allow_html=True)
                
                st.markdown("**ë¶€ì •/ìš°ë ¤**")
                for _, row in df[df['sentiment_label'] == 'negative'].nlargest(3, 'like_count').iterrows():
                    text = str(row['text'])[:100] + ('...' if len(str(row['text'])) > 100 else '')
                    st.markdown(f"""
                    <div class="comment-box negative">
                        "{text}"<br>
                        <small>ğŸ‘ {row['like_count']:,}</small>
                    </div>
                    """, unsafe_allow_html=True)
            
            st.divider()
            
            # ì•¡ì…˜ ì•„ì´í…œ
            st.markdown("### âœ… ì•¡ì…˜ ì•„ì´í…œ")
            actions = []
            if pos_pct > 50:
                actions.append("íŒ¬ ì°¸ì—¬í˜• ì½˜í…ì¸ (Q&A, íˆ¬í‘œ, ì±Œë¦°ì§€) ê¸°íšìœ¼ë¡œ engagement ê·¹ëŒ€í™”")
            if neg_pct > 20:
                actions.append("ë¶€ì • ëŒ“ê¸€ íŒ¨í„´ ë¶„ì„ í›„ FAQ ë˜ëŠ” ê³µì§€ í˜•íƒœì˜ ì„ ì œì  ì»¤ë®¤ë‹ˆì¼€ì´ì…˜")
            if topic_summaries:
                actions.append(f'"{topic_summaries[0]["keywords"][0]}" ì£¼ì œ í™•ì¥ ì½˜í…ì¸ ë¡œ ì‹œì²­ì ê´€ì‹¬ ì§€ì† ìœ ë„')
            if keywords:
                actions.append(f'"{keywords[0][0]}" í‚¤ì›Œë“œ í™œìš©í•œ ì¸ë„¤ì¼/ì œëª© A/B í…ŒìŠ¤íŠ¸')
            actions.append("ì—´ì„± íŒ¬(ë°˜ë³µ ëŒ“ê¸€ëŸ¬) ì‹ë³„ í›„ ì•°ë°°ì„œë”/VIP í”„ë¡œê·¸ë¨ íƒ€ê²ŸíŒ…")
            
            for i, action in enumerate(actions, 1):
                st.markdown(f"**{i}.** {action}")
            
            st.divider()
            st.caption("YouTube Comment Insight Report Â· Auto-generated")
            
        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


if __name__ == "__main__":
    main()
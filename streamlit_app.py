#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìœ íŠœë¸Œ ëŒ“ê¸€ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ê¸° v8.1
================================
v8.1 ì—…ë°ì´íŠ¸:
- ëŒ“ê¸€ 1000ê°œ ì´ˆê³¼ ì‹œ ë¶„ì„/ì „ì²´ ëŒ“ê¸€ ìˆ˜ í‘œì‹œ
- ê°ì„± ë¶„ë¥˜ ì •í™•ë„ ê°œì„ 
- ì˜ê²¬ ìœ í˜• ì¹´í…Œê³ ë¦¬ ì¬ì •ì˜
- UI ê°œì„  (ë¶„ì„ ê¸°ì¤€ ëª…ì‹œ, ì„¹ì…˜ ìˆœì„œ ë³€ê²½)
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import re
import io
from typing import List, Tuple, Dict, Optional
from collections import Counter, defaultdict
from datetime import datetime

# =============================================================================
# í˜ì´ì§€ ì„¤ì •
# =============================================================================
st.set_page_config(
    page_title="ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ê¸°",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# =============================================================================
# ì„¸ì…˜ ìƒíƒœ
# =============================================================================
if 'input_key' not in st.session_state:
    st.session_state.input_key = 0

# =============================================================================
# CSS
# =============================================================================
st.markdown("""
<style>
    .stApp { background-color: #f8fafc; }
    .block-container { padding: 2rem 3rem !important; max-width: 1200px !important; }
    
    .header { text-align: center; padding: 2rem 0 1.5rem 0; }
    .header h1 { color: #1e3a5f; font-size: 2rem; font-weight: 700; margin: 0 0 0.5rem 0; }
    .header p { color: #64748b; font-size: 1rem; margin: 0; }
    
    .card {
        background: white;
        border-radius: 12px;
        padding: 1.5rem;
        box-shadow: 0 1px 3px rgba(0,0,0,0.08);
        margin-bottom: 1rem;
    }
    
    .video-info-box {
        background: white;
        border-radius: 12px;
        padding: 1.5rem;
        box-shadow: 0 1px 3px rgba(0,0,0,0.08);
        margin-bottom: 1.5rem;
    }
    .video-info-row {
        display: flex;
        padding: 0.5rem 0;
        border-bottom: 1px solid #f1f5f9;
    }
    .video-info-row:last-child { border-bottom: none; }
    .video-info-label { color: #64748b; font-size: 0.85rem; min-width: 100px; font-weight: 500; }
    .video-info-value { color: #1e293b; font-size: 0.9rem; flex: 1; }
    
    .section-title {
        font-size: 1.1rem;
        font-weight: 600;
        color: #1e3a5f;
        margin: 2rem 0 1rem 0;
        padding-bottom: 0.5rem;
        border-bottom: 2px solid #e2e8f0;
    }
    
    .insight {
        background: white;
        border-left: 3px solid #1e3a5f;
        padding: 1rem 1.25rem;
        margin-bottom: 0.75rem;
        border-radius: 0 8px 8px 0;
        box-shadow: 0 1px 3px rgba(0,0,0,0.08);
    }
    .insight-title { font-weight: 600; color: #1e3a5f; font-size: 0.95rem; margin-bottom: 0.4rem; }
    .insight-desc { color: #475569; font-size: 0.9rem; line-height: 1.6; }
    .insight-action { color: #64748b; font-size: 0.85rem; font-style: italic; margin-top: 0.4rem; }
    
    .stat-box {
        background: linear-gradient(135deg, #1e3a5f 0%, #2d5a87 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
    }
    .stat-box-light {
        background: #f0f4f8;
        color: #1e3a5f;
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
    }
    .stat-value { font-size: 1.5rem; font-weight: 700; }
    .stat-label { font-size: 0.8rem; opacity: 0.9; }
    
    .lang-tag {
        display: inline-block;
        padding: 0.25rem 0.75rem;
        border-radius: 20px;
        font-size: 0.8rem;
        margin: 0.2rem;
        background: #e2e8f0;
        color: #475569;
    }
    .lang-tag.primary { background: #1e3a5f; color: white; }
    
    .category-card {
        background: white;
        border-radius: 10px;
        padding: 1rem;
        margin-bottom: 0.75rem;
        box-shadow: 0 1px 3px rgba(0,0,0,0.06);
        border-left: 4px solid #1e3a5f;
    }
    .category-title { font-weight: 600; color: #1e3a5f; font-size: 0.9rem; }
    .category-pct { color: #64748b; font-size: 0.85rem; }
    .category-sample { color: #475569; font-size: 0.85rem; font-style: italic; margin-top: 0.5rem; }
    
    .journey-stage {
        padding: 0.75rem 1rem;
        border-radius: 8px;
        margin-bottom: 0.5rem;
        display: flex;
        justify-content: space-between;
        align-items: center;
    }
    .journey-awareness { background: #dbeafe; color: #1e40af; }
    .journey-interest { background: #dcfce7; color: #166534; }
    .journey-consideration { background: #fef3c7; color: #92400e; }
    .journey-intent { background: #fce7f3; color: #9d174d; }
    .journey-experience { background: #f3e8ff; color: #7c3aed; }
    
    .comment {
        background: #f8fafc;
        padding: 0.9rem 1rem;
        border-radius: 8px;
        margin-bottom: 0.5rem;
        border-left: 3px solid #cbd5e1;
    }
    .comment.pos { border-color: #1e3a5f; }
    .comment.neg { border-color: #94a3b8; }
    .comment-text { color: #334155; font-size: 0.88rem; line-height: 1.5; }
    .comment-likes { color: #94a3b8; font-size: 0.8rem; margin-top: 0.3rem; }
    
    .warning-box {
        background: #fef3c7;
        border: 1px solid #f59e0b;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .info-box {
        background: #dbeafe;
        border: 1px solid #3b82f6;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    
    .footer { text-align: center; padding: 2rem 0; color: #94a3b8; font-size: 0.8rem; }
    
    #MainMenu, footer, .stDeployButton {display: none;}
    
    .stButton > button {
        background: #1e3a5f;
        color: white;
        border: none;
        padding: 0.7rem 2rem;
        font-weight: 600;
        border-radius: 8px;
    }
    .stButton > button:hover { background: #2d5a87; }
    
    .stDownloadButton > button {
        background: white;
        color: #1e3a5f;
        border: 2px solid #1e3a5f;
    }
    .stDownloadButton > button:hover { background: #1e3a5f; color: white; }
    
    .stTextInput > div > div > input {
        border-radius: 8px;
        border: 1px solid #e2e8f0;
        padding: 0.7rem 1rem;
    }
    
    [data-testid="stMetricValue"] { font-size: 1.5rem; color: #1e3a5f; }
    [data-testid="stMetricLabel"] { color: #64748b; }
</style>
""", unsafe_allow_html=True)

# =============================================================================
# ì„¤ì • & ìƒìˆ˜
# =============================================================================
CONFIG = {"max_comments": 1000, "top_keywords_count": 15}

STOPWORDS = set(['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ë¡œ', 'ìœ¼ë¡œ',
    'í•˜ê³ ', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ë°', 'ë“±', 'ë”', 'ë§‰', 'ì¢€', 'ì´ì œ',
    'ë‚˜', 'ë„ˆ', 'ìš°ë¦¬', 'ì €', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê²ƒ', 'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°',
    'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê°™ë‹¤', 'ë³´ë‹¤', 'ì•Œë‹¤', 'ì‹¶ë‹¤', 'ì£¼ë‹¤', 'ë³´ë‹¤',
    'í•˜ëŠ”', 'í•˜ë©´', 'í•´ì„œ', 'í–ˆë‹¤', 'í•œë‹¤', 'í• ', 'í•¨', 'ë˜ëŠ”', 'ë˜ë©´', 'ëë‹¤', 'ëœë‹¤', 'í•˜ê²Œ', 'í•´ìš”', 'í•©ë‹ˆë‹¤',
    'ìˆëŠ”', 'ìˆìœ¼ë©´', 'ìˆê³ ', 'ìˆì–´ì„œ', 'ìˆì—ˆë‹¤', 'ìˆì„', 'ìˆìŒ', 'ìˆì–´ìš”', 'ìˆìŠµë‹ˆë‹¤',
    'ê²ƒ', 'ê±°', 'ìˆ˜', 'ë•Œ', 'ì¤‘', 'ë‚´', 'ë…„', 'ì›”', 'ì¼', 'ë²ˆ', 'ë¶„', 'ê²Œ', 'ë°', 'ë­', 'ì™œ', 'ì–´ë–»ê²Œ',
    'ì˜ìƒ', 'ëŒ“ê¸€', 'ë™ì˜ìƒ', 'ìœ íŠœë¸Œ', 'ì±„ë„', 'êµ¬ë…', 'ì¢‹ì•„ìš”', 'ì‹œì²­', 'ì§„ì§œ', 'ë„ˆë¬´', 'ì •ë§', 'ì™„ì „',
    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by',
    'i', 'me', 'my', 'you', 'your', 'he', 'she', 'it', 'we', 'they', 'this', 'that', 'and', 'but', 'or', 'so',
    'video', 'comment', 'youtube', 'channel', 'subscribe', 'like', 'just', 'really', 'very', 'much', 'what', 'how'])

POSITIVE_WORDS = {'ì¢‹ë‹¤', 'ì¢‹ì•„', 'ì¢‹ë„¤', 'ì¢‹ì€', 'ì¢‹ì•˜', 'ì¢‹ìŒ', 'ì¢‹ì•„ìš”', 'ì¢‹ìŠµë‹ˆë‹¤', 'ìµœê³ ', 'ìµœê³ ë‹¤', 'ìµœê³ ì•¼', 'ìµœê³ ì˜ˆìš”', 'ìµœê³ ì„',
    'ëŒ€ë°•', 'ëŒ€ë°•ì´ë‹¤', 'ë©‹ì§€ë‹¤', 'ë©‹ì ¸', 'ë©‹ìˆë‹¤', 'ë©‹ìˆì–´', 'ë©‹ì§', 'ë©‹ì§„', 'ì˜ˆì˜ë‹¤', 'ì˜ˆë»', 'ì˜ˆì¨', 'ì´ì˜ë‹¤', 'ì´ë»', 'ì˜ˆìœ',
    'ì‚¬ë‘', 'ì‚¬ë‘í•´', 'ì‚¬ë‘í•´ìš”', 'ì‚¬ë‘í•©ë‹ˆë‹¤', 'ê°ì‚¬', 'ê°ì‚¬í•´ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ê³ ë§ˆì›Œ', 'ê³ ë§™ìŠµë‹ˆë‹¤',
    'í–‰ë³µ', 'í–‰ë³µí•´', 'ê¸°ì˜ë‹¤', 'ì¦ê²ë‹¤', 'ê¸°ëŒ€', 'ê¸°ëŒ€ëœë‹¤', 'ê¸°ëŒ€ë¼', 'ì‘ì›', 'ì‘ì›í•´', 'í™”ì´íŒ…', 'íŒŒì´íŒ…', 'í˜ë‚´',
    'í›Œë¥­', 'ì™„ë²½', 'ê°ë™', 'ì„¤ë ˜', 'ì„¤ë ˆ', 'ì¬ë°Œ', 'ì¬ë°Œë‹¤', 'ì¬ë¯¸ìˆ', 'ì›ƒê¸°ë‹¤', 'ì›ƒê²¨', 'íë§', 'ê·€ì—½', 'ê·€ì—¬ì›Œ', 'ê·€ì—¬ìš´', 'ê·€ì—¼',
    'ì˜ìƒ', 'ì˜ìƒê²¼', 'ì¡´ì˜', 'ì¡´ì˜ˆ', 'ì§±', 'ì©”ì–´', 'ì©ë‹¤', 'ë¯¸ì³¤', 'ë¯¸ì³¤ë‹¤', 'ëŒ€ë‹¨', 'ë†€ë', 'ì‹ ê¸°', 'ë ˆì „ë“œ',
    'ì¸ì •', 'ì¶”ì²œ', 'ê°“', 'ì¡´ê²½', 'ì²œì¬', 'ì•„ë¦„ë‹µ', 'í™˜ìƒì ', 'ì—­ì‹œ', 'ë¯¿ê³ ë³´ëŠ”', 'ì°', 'ê¿€ì¼', 'í•µì¼', 'ì¡´ì¼', 'ì†Œë¦„', 'ê°íƒ„', 'ê³µê°',
    # v8.1 ì¶”ê°€: ê´‘ê³ /ëª¨ë¸ ê´€ë ¨ ê¸ì • í‘œí˜„
    'ì†Œí™”', 'ë‹¤ì–‘', 'ë§¤ë ¥', 'ë¶„ìœ„ê¸°', 'ì•„ìš°ë¼', 'ì¹´ë¦¬ìŠ¤ë§ˆ', 'ë¹„ì£¼ì–¼', 'í”¼ì§€ì»¬', 'ì²­ìˆœ', 'ì„¹ì‹œ', 'ìˆì–¸ë‹ˆ', 'ê±¸í¬ëŸ¬ì‹œ',
    'ì°°ë–¡', 'ì–´ìš¸ë¦¬', 'ì–´ìš¸ë¦°ë‹¤', 'ì˜ì–´ìš¸', 'ì‹±í¬ë¡œ', 'ì¼€ë¯¸', 'í…ì…˜', 'ì„¼ìŠ¤', 'ìœ ë¨¸', 'ì‘ì •', 'ë³¸ì—…', 'ì¥ì¸', 'í”„ë¡œ',
    'í€„ë¦¬í‹°', 'ì™„ì„±ë„', 'ê³ ê¸‰', 'ì„¸ë ¨', 'ê°ê°', 'ê°ì„±', 'í™', 'íŠ¸ë Œë””', 'ì‹ ì„ ', 'ì°¸ì‹ ', 'ê¸°ë°œ', 'ë…íŠ¹', 'ê°œì„±',
    'ì¤‘ë…', 'ê³„ì†', 'ë°˜ë³µ', 'ë˜', 'ë‹¤ì‹œ', 'ëª‡ë²ˆì§¸', 'ë£¨í”„', 'ëŒë ¤ë´„', 'í‚¬ë§', 'í¬ì¸íŠ¸', 'ì„íŒ©íŠ¸',
    'good', 'great', 'best', 'love', 'amazing', 'awesome', 'beautiful', 'excellent', 'fantastic', 'perfect', 'happy',
    'incredible', 'brilliant', 'wow', 'omg', 'fire', 'goat', 'queen', 'king', 'icon', 'slay', 'legend', 'cute', 'pretty'}

NEGATIVE_WORDS = {'ì‹«ë‹¤', 'ì‹«ì–´', 'ì‹«ìŒ', 'ë³„ë¡œ', 'ë³„ë£¨', 'ìµœì•…', 'ì‹¤ë§', 'ì‹¤ë§í–ˆ', 'ì§œì¦', 'ì§œì¦ë‚˜', 'ì§œì¦ë‚¨',
    'í™”ë‚˜', 'í™”ë‚¨', 'ë‹µë‹µ', 'ë¶ˆì¾Œ', 'ìŠ¬í”„', 'ìŠ¬í¼', 'ìš°ìš¸', 'ìš°ìš¸í•´',
    'ì•„ì‰½', 'ì•„ì‰¬ì›Œ', 'ê±±ì •', 'ë¶ˆì•ˆ', 'í˜ë“¤', 'í”¼ê³¤', 'ë‚˜ì˜', 'ë‚˜ë¹ ', 'ëª»í•˜', 'ëª»í•¨', 'í›„íšŒ', 'í˜ì˜¤', 'ì—­ê²¹', 
    'ì§€ë£¨', 'ë…¸ì¼', 'ì¬ë¯¸ì—†', 'ë§í–ˆ', 'ë§í•¨', 'ì“°ë ˆê¸°', 'ë¶ˆí¸', 'ë¹„ì¶”', 'ê·¹í˜', 'í­ë§', 'ì‹¤íŒ¨',
    # ëª…í™•í•œ ë¶€ì • í‘œí˜„ë§Œ ìœ ì§€ (ëª¨í˜¸í•œ í‘œí˜„ ì œê±°)
    'bad', 'worst', 'hate', 'terrible', 'awful', 'sad', 'angry', 'disappointed', 'boring', 'fail', 'trash', 'cringe', 'sucks'}

POSITIVE_EMOJIS = set('ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ¤£ğŸ˜‚ğŸ˜ŠğŸ˜‡ğŸ¥°ğŸ˜ğŸ¤©ğŸ˜˜ğŸ‘ğŸ‘ğŸ™ŒğŸ’ªâœ¨ğŸŒŸâ­ğŸ’–ğŸ’—â¤ğŸ§¡ğŸ’›ğŸ’šğŸ’™ğŸ’œğŸ’ğŸ”¥ğŸ’¯ğŸ‰ğŸ‘‘ğŸ’ğŸ†ğŸ˜ğŸ¤—ğŸ¥³â¤ï¸')
NEGATIVE_EMOJIS = set('ğŸ˜¢ğŸ˜­ğŸ˜¤ğŸ˜ ğŸ˜¡ğŸ¤¬ğŸ’”ğŸ‘ğŸ™„ğŸ˜’ğŸ˜ğŸ˜”ğŸ˜ŸğŸ™ğŸ˜£ğŸ˜–ğŸ˜«ğŸ˜©ğŸ˜±ğŸ¤®ğŸ¤¢')

# =============================================================================
# [NEW] ì˜ê²¬ ìœ í˜• ë¶„ë¥˜ í‚¤ì›Œë“œ ì‚¬ì „ (v8.1 ìˆ˜ì •)
# =============================================================================
OPINION_TAXONOMY = {
    'product_service': {
        'name': 'ì œí’ˆ/ì„œë¹„ìŠ¤',
        'keywords': ['ì œí’ˆ', 'ì„œë¹„ìŠ¤', 'ì•±', 'ì–´í”Œ', 'í”„ë¡œê·¸ë¨', 'ì†Œí”„íŠ¸ì›¨ì–´', 'ê¸°ëŠ¥', 'ì—…ë°ì´íŠ¸', 'ë²„ì „', 'ì¶œì‹œ',
                    'ê°€ê²©', 'ìš”ê¸ˆ', 'êµ¬ë…', 'ë¬´ë£Œ', 'ìœ ë£Œ', 'í”Œëœ', 'pro', 'premium', 'ê²°ì œ', 'í™˜ë¶ˆ', 'í’ˆì§ˆ',
                    'product', 'service', 'app', 'feature', 'update', 'price', 'subscription', 'ì‚¬ìš©', 'ì´ìš©',
                    'ì¦ê¶Œ', 'ì€í–‰', 'ì¹´ë“œ', 'ë³´í—˜', 'í†µì‹ ', 'ë°°ë‹¬', 'ì‡¼í•‘', 'í”Œë«í¼'],
        'color': '#10b981'
    },
    'brand': {
        'name': 'ë¸Œëœë“œ',
        'keywords': ['íšŒì‚¬', 'ê¸°ì—…', 'ë¸Œëœë“œ', 'êµ¬ê¸€', 'google', 'ì• í”Œ', 'apple', 'ì‚¼ì„±', 'samsung', 'ë„¤ì´ë²„', 'ì¹´ì¹´ì˜¤',
                    'í˜„ëŒ€', 'lg', 'sk', 'ë¡¯ë°', 'ì‹ ì„¸ê³„', 'cj', 'í•œí™”', 'ëŒ€ê¸°ì—…', 'ìŠ¤íƒ€íŠ¸ì—…', 'ê´‘ê³ ì£¼', 'í˜‘ì°¬ì‚¬',
                    'nike', 'ë‚˜ì´í‚¤', 'adidas', 'ì•„ë””ë‹¤ìŠ¤', 'ë£¨ì´ë¹„í†µ', 'ìƒ¤ë„¬', 'êµ¬ì°Œ', 'brand', 'company'],
        'color': '#f59e0b'
    },
    'market_social': {
        'name': 'ì‹œì¥/ì‚¬íšŒì  ì˜í–¥',
        'keywords': ['ì¼ìë¦¬', 'ì§ì—…', 'ëŒ€ì²´', 'ì‹¤ì—…', 'ë¯¸ë˜', 'ìœ„í—˜', 'ìœ¤ë¦¬', 'ê·œì œ', 'ë²•', 'ì •ì±…', 'ì €ì‘ê¶Œ',
                    'í”„ë¼ì´ë²„ì‹œ', 'ê°œì¸ì •ë³´', 'ë³´ì•ˆ', 'ì‚¬íšŒ', 'ì˜í–¥', 'ë³€í™”', 'íŠ¸ë Œë“œ', 'ì„¸ëŒ€', 'ë¬¸í™”', 'ë…¼ë€',
                    'job', 'future', 'risk', 'regulation', 'ethics', 'trend', 'ê²½ì œ', 'ì‹œì¥', 'ì—…ê³„'],
        'color': '#ef4444'
    },
    'model_person': {
        'name': 'ëª¨ë¸/ì¶œì—°ì',
        'keywords': ['ëª¨ë¸', 'ë°°ìš°', 'ì—°ì˜ˆì¸', 'ì•„ì´ëŒ', 'ê°€ìˆ˜', 'ì¶œì—°', 'ìºìŠ¤íŒ…', 'ì–¼êµ´', 'ì™¸ëª¨', 'ìŠ¤íƒ€ì¼', 'íŒ¨ì…˜',
                    'ì—°ê¸°', 'í‘œì •', 'ëª©ì†Œë¦¬', 'ë§¤ë ¥', 'ë¶„ìœ„ê¸°', 'ì´ë¯¸ì§€', 'ë¹„ì£¼ì–¼', 'í”¼ì§€ì»¬', 'ì•„ìš°ë¼', 'ì¹´ë¦¬ìŠ¤ë§ˆ',
                    'íŒ¬', 'ë•ì§ˆ', 'ìµœì• ', 'ì…€ëŸ½', 'celebrity', 'idol', 'actor', 'actress', 'ê´‘ê³ ëª¨ë¸'],
        'color': '#ec4899'
    },
    'visual_aesthetic': {
        'name': 'ì˜ìƒë¯¸/ì‹¬ë¯¸ì„±',
        'keywords': ['ì˜ìƒë¯¸', 'í™”ì§ˆ', 'ìƒ‰ê°', 'ì¡°ëª…', 'ì´¬ì˜', 'êµ¬ë„', 'í¸ì§‘', 'ì—°ì¶œ', 'ê°ë…', 'cg', 'ê·¸ë˜í”½', 'íš¨ê³¼',
                    'ì•„ë¦„ë‹µ', 'ì˜ˆì˜', 'ë©‹ìˆ', 'í™”ë ¤', 'ê³ ê¸‰', 'ì„¸ë ¨', 'ê°ê°', 'í€„ë¦¬í‹°', 'ì™„ì„±ë„', 'ë””ìì¸', 'ë¯¸ì ',
                    'beautiful', 'aesthetic', 'visual', 'quality', 'cinematic', 'ë°°ê²½', 'ì¥ë©´', 'ì•µê¸€', 'ë¬´ë“œ'],
        'color': '#8b5cf6'
    },
    'fun_entertainment': {
        'name': 'ì¬ë¯¸ìš”ì†Œ',
        'keywords': ['ì¬ë°Œ', 'ì¬ë¯¸', 'ì›ƒê¸°', 'ì›ƒê¸´', 'ì›ƒìŒ', 'ìœ ë¨¸', 'ì„¼ìŠ¤', 'í‚¬ë§í¬ì¸íŠ¸', 'ì¤‘ë…', 'ê³„ì†', 'ë°˜ë³µ',
                    'ê¿€ì¼', 'í•µì¼', 'ì¡´ì¼', 'ë…¸ì¼', 'funny', 'fun', 'hilarious', 'lol', 'lmao', 'ê°œê·¸', 'ì½”ë¯¸ë””',
                    'ë³‘ë§›', 'ì°°ë–¡', 'í¬ì¸íŠ¸', 'ì„íŒ©íŠ¸', 'ì‹ ì„ ', 'ì°¸ì‹ ', 'ê¸°ë°œ', 'ì•„ì´ë””ì–´', 'ì»¨ì…‰', 'ìŠ¤í† ë¦¬'],
        'color': '#06b6d4'
    }
}

# =============================================================================
# [NEW] êµ¬ë§¤ ì—¬ì • ë‹¨ê³„ í‚¤ì›Œë“œ ì‚¬ì „
# =============================================================================
JOURNEY_STAGES = {
    'awareness': {
        'name': 'ì¸ì§€ (Awareness)',
        'keywords': ['ë­ì•¼', 'ë­”ì§€', 'ì²˜ìŒ', 'ì•Œê²Œ', 'ë“¤ì–´ë´¤', 'ëª°ë', 'ì´ëŸ°ê²Œ', 'ì‹ ê¸°', 'ì˜¤', 'ì™€', 'í—',
                    'what is', 'first time', 'never knew', 'discover', 'ì¡´ì¬', 'ìˆì—ˆ', 'ìƒˆë¡œìš´'],
        'color': '#dbeafe'
    },
    'interest': {
        'name': 'ê´€ì‹¬ (Interest)',
        'keywords': ['ê¶ê¸ˆ', 'ì•Œê³ ì‹¶', 'ë” ì•Œë ¤', 'ì–´ë–»ê²Œ', 'ë°©ë²•', 'ê°€ëŠ¥', 'ë ê¹Œ', 'í•  ìˆ˜', 'í•´ë³¼',
                    'curious', 'how to', 'want to know', 'interesting', 'ê´€ì‹¬', 'ì°¾ì•„', 'ê²€ìƒ‰'],
        'color': '#dcfce7'
    },
    'consideration': {
        'name': 'ê³ ë ¤ (Consideration)',
        'keywords': ['ë¹„êµ', 'ì°¨ì´', 'ë­ê°€ ë”', 'ì–´ë–¤ê²Œ', 'ì¶”ì²œ', 'ê³ ë¯¼', 'vs', 'ëŒ€', 'ì¥ë‹¨ì ', 'ë¹„ìš©',
                    'compare', 'difference', 'which', 'recommend', 'pros cons', 'ì„ íƒ', 'ê²°ì •'],
        'color': '#fef3c7'
    },
    'intent': {
        'name': 'êµ¬ë§¤ì˜ë„ (Intent)',
        'keywords': ['ì‚¬ì•¼', 'ì‚´ê¹Œ', 'êµ¬ë§¤', 'ê²°ì œ', 'ì‹ ì²­', 'ê°€ì…', 'ì‹œì‘', 'ì¨ë³¼', 'í•´ë³¼ê¹Œ', 'ì§ˆëŸ¬',
                    'buy', 'purchase', 'subscribe', 'sign up', 'start', 'ëˆ', 'íˆ¬ì', 'ì§€ë¥¼ê¹Œ'],
        'color': '#fce7f3'
    },
    'experience': {
        'name': 'ê²½í—˜ (Experience)',
        'keywords': ['ì¨ë´¤', 'ì‚¬ìš©í•´ë´¤', 'í•´ë´¤ëŠ”ë°', 'ê²½í—˜', 'í›„ê¸°', 'ì†”ì§íˆ', 'ì‹¤ì œë¡œ', 'ì§ì ‘', 'ê²°ê³¼',
                    'used', 'tried', 'experience', 'review', 'actually', 'ëŠë‚Œ', 'ì²´ê°', 'ë§Œì¡±', 'ë¶ˆë§Œ'],
        'color': '#f3e8ff'
    }
}

# =============================================================================
# [NEW] ê¸°ëŒ€ ìš”ì¸ / ë¶ˆì•ˆ ìš”ì¸ í‚¤ì›Œë“œ
# =============================================================================
EXPECTATION_KEYWORDS = ['ê¸°ëŒ€', 'ê¸°ëŒ€ëœë‹¤', 'ê¸°ë‹¤ë ¤', 'ì–¼ë¥¸', 'ë¹¨ë¦¬', 'ê³§', 'ì–¸ì œ', 'ì¶œì‹œ', 'ì—…ë°ì´íŠ¸', 
                        'excited', 'cant wait', 'looking forward', 'hope', 'í¬ë§', 'ë°”ëŒ', 'ì›í•´']
ANXIETY_KEYWORDS = ['ê±±ì •', 'ë¶ˆì•ˆ', 'ë¬´ì„­', 'ë‘ë µ', 'ìœ„í—˜', 'ë¬¸ì œ', 'ìš°ë ¤', 'ì—¼ë ¤', 'ì¡°ì‹¬', 'ì£¼ì˜',
                   'worried', 'concern', 'scary', 'dangerous', 'risk', 'afraid', 'ëŒ€ì²´', 'ì‚¬ë¼ì§ˆ']

# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================================================
def extract_video_id(url):
    if not url: return None
    patterns = [r'(?:youtube\.com/watch\?v=|youtu\.be/|youtube\.com/embed/|youtube\.com/shorts/)([a-zA-Z0-9_-]{11})', r'[?&]v=([a-zA-Z0-9_-]{11})']
    for p in patterns:
        m = re.search(p, url)
        if m: return m.group(1)
    return url if re.match(r'^[a-zA-Z0-9_-]{11}$', url) else None

def clean_text(text):
    if not isinstance(text, str): return ""
    text = re.sub(r'http[s]?://\S+', '', text)
    text = re.sub(r'<[^>]+>', '', text)
    return re.sub(r'\s+', ' ', text).strip()

def preprocess(text):
    text = clean_text(text).lower()
    text = re.compile("[" + u"\U0001F600-\U0001F64F" + u"\U0001F300-\U0001F5FF" + u"\U0001F680-\U0001F6FF" + u"\U0001F1E0-\U0001F1FF" + u"\U00002702-\U000027B0" + "]+", re.UNICODE).sub('', text)
    text = re.sub(r'[^\w\sê°€-í£a-zA-Z0-9]', ' ', text)
    return ' '.join([t for t in text.split() if t not in STOPWORDS and len(t) > 1])

def format_date(d):
    return f"{d[:4]}ë…„ {d[4:6]}ì›” {d[6:8]}ì¼" if d and len(d) == 8 else "ì •ë³´ ì—†ìŒ"

def format_num(n):
    try:
        n = int(n) if n else 0
        if n >= 100000000: return f"{n/100000000:.1f}ì–µ"
        if n >= 10000: return f"{n/10000:.1f}ë§Œ"
        if n >= 1000: return f"{n/1000:.1f}ì²œ"
        return f"{n:,}"
    except: return "0"

# =============================================================================
# [NEW] ì–¸ì–´ ê°ì§€ í•¨ìˆ˜
# =============================================================================
def detect_language(text: str) -> str:
    """ëŒ“ê¸€ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€"""
    if not text or len(text.strip()) < 3:
        return 'unknown'
    
    try:
        from langdetect import detect, LangDetectException
        try:
            lang = detect(text)
            # ì£¼ìš” ì–¸ì–´ ë§¤í•‘
            lang_map = {
                'ko': 'í•œêµ­ì–´', 'en': 'ì˜ì–´', 'ja': 'ì¼ë³¸ì–´', 'zh-cn': 'ì¤‘êµ­ì–´', 'zh-tw': 'ì¤‘êµ­ì–´',
                'es': 'ìŠ¤í˜ì¸ì–´', 'pt': 'í¬ë¥´íˆ¬ê°ˆì–´', 'fr': 'í”„ë‘ìŠ¤ì–´', 'de': 'ë…ì¼ì–´',
                'ru': 'ëŸ¬ì‹œì•„ì–´', 'ar': 'ì•„ëì–´', 'hi': 'íŒë””ì–´', 'th': 'íƒœêµ­ì–´', 'vi': 'ë² íŠ¸ë‚¨ì–´',
                'id': 'ì¸ë„ë„¤ì‹œì•„ì–´', 'ms': 'ë§ë ˆì´ì–´', 'tl': 'í•„ë¦¬í•€ì–´'
            }
            return lang_map.get(lang, lang)
        except LangDetectException:
            return 'unknown'
    except ImportError:
        # langdetect ì—†ìœ¼ë©´ ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±
        korean = len(re.findall(r'[ê°€-í£]', text))
        english = len(re.findall(r'[a-zA-Z]', text))
        japanese = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF]', text))
        chinese = len(re.findall(r'[\u4e00-\u9fff]', text))
        
        scores = {'í•œêµ­ì–´': korean, 'ì˜ì–´': english, 'ì¼ë³¸ì–´': japanese, 'ì¤‘êµ­ì–´': chinese}
        max_lang = max(scores, key=scores.get)
        return max_lang if scores[max_lang] > 0 else 'unknown'

def analyze_by_language(df: pd.DataFrame) -> Dict:
    """ì–¸ì–´ë³„ ë¶„ì„ ìˆ˜í–‰"""
    if 'language' not in df.columns:
        return {}
    
    results = {}
    lang_counts = df['language'].value_counts()
    total = len(df)
    
    for lang in lang_counts.index:
        if lang == 'unknown':
            continue
        
        lang_df = df[df['language'] == lang]
        count = len(lang_df)
        
        if count < 5:  # ìµœì†Œ 5ê°œ ì´ìƒë§Œ ë¶„ì„
            continue
        
        # ê°ì„± ë¶„í¬
        sentiment_dist = lang_df['sentiment'].value_counts().to_dict()
        pos_pct = sentiment_dist.get('positive', 0) / count * 100
        neg_pct = sentiment_dist.get('negative', 0) / count * 100
        
        # í‚¤ì›Œë“œ
        texts = lang_df['text'].tolist()
        keywords = extract_keywords(texts, 5)
        
        results[lang] = {
            'count': count,
            'percentage': count / total * 100,
            'positive_pct': pos_pct,
            'negative_pct': neg_pct,
            'neutral_pct': 100 - pos_pct - neg_pct,
            'keywords': keywords,
            'sentiment_dist': sentiment_dist
        }
    
    return results

def generate_language_insight(lang_analysis: Dict) -> str:
    """ì–¸ì–´ë³„ ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    if not lang_analysis:
        return ""
    
    insights = []
    sorted_langs = sorted(lang_analysis.items(), key=lambda x: x[1]['count'], reverse=True)
    
    # ì£¼ìš” ì–¸ì–´ íŒŒì•…
    if sorted_langs:
        main_lang = sorted_langs[0][0]
        main_data = sorted_langs[0][1]
        insights.append(f"â–¸ **ì£¼ìš” ì‹œì²­ì¸µ**: {main_lang} ì‚¬ìš©ìê°€ ì „ì²´ì˜ {main_data['percentage']:.0f}%ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤.")
        
        # ì–¸ì–´ë³„ ê°ì„± ì°¨ì´
        if len(sorted_langs) > 1:
            sentiments = [(lang, data['positive_pct']) for lang, data in sorted_langs[:3]]
            most_positive = max(sentiments, key=lambda x: x[1])
            if most_positive[1] > main_data['positive_pct'] + 10:
                insights.append(f"â–¸ **ì–¸ì–´ë³„ ì˜¨ë„ì°¨**: {most_positive[0]} ì‚¬ìš©ìê°€ ê°€ì¥ ê¸ì •ì ({most_positive[1]:.0f}%)ìœ¼ë¡œ, í•´ë‹¹ ì‹œì¥ì—ì„œì˜ ë°˜ì‘ì´ ì¢‹ìŠµë‹ˆë‹¤.")
        
        # ê¸€ë¡œë²Œ í™•ì¥ ê°€ëŠ¥ì„±
        non_main_pct = 100 - main_data['percentage']
        if non_main_pct > 30:
            insights.append(f"â–¸ **ê¸€ë¡œë²Œ ê´€ì‹¬ë„**: í•´ì™¸ ì‹œì²­ì ë¹„ìœ¨ì´ {non_main_pct:.0f}%ë¡œ, ë‹¤êµ­ì–´ ì½˜í…ì¸  ì „ëµì´ ìœ íš¨í•©ë‹ˆë‹¤.")
    
    return '\n\n'.join(insights)

# =============================================================================
# [NEW] ì˜ê²¬ ìœ í˜• ë¶„ë¥˜ í•¨ìˆ˜
# =============================================================================
def classify_opinion_type(text: str) -> List[str]:
    """ëŒ“ê¸€ì„ ì˜ê²¬ ìœ í˜•ìœ¼ë¡œ ë¶„ë¥˜ (ë³µìˆ˜ ê°€ëŠ¥)"""
    if not text:
        return []
    
    text_lower = text.lower()
    categories = []
    
    for cat_id, cat_info in OPINION_TAXONOMY.items():
        for keyword in cat_info['keywords']:
            if keyword in text_lower:
                categories.append(cat_id)
                break
    
    return categories if categories else ['fun_entertainment']  # ê¸°ë³¸ê°’: ì¬ë¯¸ìš”ì†Œ

def analyze_opinion_taxonomy(df: pd.DataFrame) -> Dict:
    """ì˜ê²¬ ìœ í˜•ë³„ ë¶„ì„"""
    if 'categories' not in df.columns:
        return {}
    
    results = {}
    total = len(df)
    
    for cat_id, cat_info in OPINION_TAXONOMY.items():
        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ë¥¼ í¬í•¨í•˜ëŠ” ëŒ“ê¸€
        cat_df = df[df['categories'].apply(lambda x: cat_id in x)]
        count = len(cat_df)
        
        if count == 0:
            continue
        
        # ê°ì„± ë¶„í¬
        sentiment_dist = cat_df['sentiment'].value_counts().to_dict()
        pos_pct = sentiment_dist.get('positive', 0) / count * 100
        neg_pct = sentiment_dist.get('negative', 0) / count * 100
        
        # ëŒ€í‘œ ëŒ“ê¸€ (ì¢‹ì•„ìš” ìƒìœ„)
        top_comment = cat_df.nlargest(1, 'like_count')
        sample = top_comment['text'].values[0][:100] if len(top_comment) > 0 else ""
        
        results[cat_id] = {
            'name': cat_info['name'],
            'count': count,
            'percentage': count / total * 100,
            'positive_pct': pos_pct,
            'negative_pct': neg_pct,
            'sample_comment': sample,
            'color': cat_info['color']
        }
    
    return results

def generate_taxonomy_insight(taxonomy_analysis: Dict) -> str:
    """ì˜ê²¬ ìœ í˜• ë¶„ì„ ì¸ì‚¬ì´íŠ¸"""
    if not taxonomy_analysis:
        return ""
    
    insights = []
    sorted_cats = sorted(taxonomy_analysis.items(), key=lambda x: x[1]['count'], reverse=True)
    
    if sorted_cats:
        top_cat = sorted_cats[0]
        insights.append(f"â–¸ **ì£¼ìš” ê´€ì‹¬ì‚¬**: ì‹œì²­ìë“¤ì€ '{top_cat[1]['name']}'ì— ê°€ì¥ ë§ì€ ì˜ê²¬ì„ ë‚¨ê²¼ìŠµë‹ˆë‹¤ ({top_cat[1]['percentage']:.0f}%).")
        
        # ë¶€ì • ë¹„ìœ¨ ë†’ì€ ì¹´í…Œê³ ë¦¬
        negative_cats = [(cat_id, data) for cat_id, data in sorted_cats if data['negative_pct'] > 30]
        if negative_cats:
            neg_cat = max(negative_cats, key=lambda x: x[1]['negative_pct'])
            insights.append(f"â–¸ **ì£¼ì˜ í•„ìš” ì˜ì—­**: '{neg_cat[1]['name']}' ê´€ë ¨ ì˜ê²¬ ì¤‘ {neg_cat[1]['negative_pct']:.0f}%ê°€ ë¶€ì •ì ì…ë‹ˆë‹¤. í•´ë‹¹ ì˜ì—­ì˜ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì‹œì¥/ì‚¬íšŒì  ì˜í–¥ ì–¸ê¸‰ ì‹œ
        if 'market_social' in taxonomy_analysis and taxonomy_analysis['market_social']['percentage'] > 15:
            insights.append(f"â–¸ **ì‚¬íšŒì  ê´€ì‹¬**: ì‹œì²­ìë“¤ì´ ì‹œì¥/ì‚¬íšŒì  ì˜í–¥ì— ëŒ€í•´ í™œë°œíˆ ë…¼ì˜ ì¤‘ì…ë‹ˆë‹¤. PR ë©”ì‹œì§€ì— 'ì±…ì„ê° ìˆëŠ” ê¸°ìˆ ' í”„ë ˆì´ë°ì„ ê³ ë ¤í•˜ì„¸ìš”.")
    
    return '\n\n'.join(insights)

# =============================================================================
# [NEW] êµ¬ë§¤ ì—¬ì • ë¶„ì„ í•¨ìˆ˜
# =============================================================================
def classify_journey_stage(text: str) -> str:
    """ëŒ“ê¸€ì„ êµ¬ë§¤ ì—¬ì • ë‹¨ê³„ë¡œ ë¶„ë¥˜"""
    if not text:
        return 'unknown'
    
    text_lower = text.lower()
    stage_scores = {}
    
    for stage_id, stage_info in JOURNEY_STAGES.items():
        score = sum(1 for kw in stage_info['keywords'] if kw in text_lower)
        stage_scores[stage_id] = score
    
    max_stage = max(stage_scores, key=stage_scores.get)
    return max_stage if stage_scores[max_stage] > 0 else 'unknown'

def analyze_journey_stages(df: pd.DataFrame) -> Dict:
    """êµ¬ë§¤ ì—¬ì • ë‹¨ê³„ë³„ ë¶„ì„"""
    if 'journey_stage' not in df.columns:
        return {}
    
    results = {}
    total = len(df)
    
    for stage_id, stage_info in JOURNEY_STAGES.items():
        stage_df = df[df['journey_stage'] == stage_id]
        count = len(stage_df)
        
        results[stage_id] = {
            'name': stage_info['name'],
            'count': count,
            'percentage': count / total * 100,
            'color': stage_info['color']
        }
    
    return results

def analyze_expectation_anxiety(df: pd.DataFrame) -> Dict:
    """ê¸°ëŒ€ ìš”ì¸ vs ë¶ˆì•ˆ ìš”ì¸ ë¶„ì„"""
    results = {'expectation': [], 'anxiety': []}
    
    for _, row in df.iterrows():
        text = str(row['text']).lower()
        
        # ê¸°ëŒ€ ìš”ì¸
        for kw in EXPECTATION_KEYWORDS:
            if kw in text:
                results['expectation'].append(row['text'][:100])
                break
        
        # ë¶ˆì•ˆ ìš”ì¸
        for kw in ANXIETY_KEYWORDS:
            if kw in text:
                results['anxiety'].append(row['text'][:100])
                break
    
    return {
        'expectation_count': len(results['expectation']),
        'anxiety_count': len(results['anxiety']),
        'expectation_samples': results['expectation'][:3],
        'anxiety_samples': results['anxiety'][:3]
    }

# =============================================================================
# [NEW] ë§ˆì¼€íŒ… ì „ëµ ì¸ì‚¬ì´íŠ¸ ìƒì„± (ê³ ë„í™”)
# =============================================================================
def generate_marketing_strategy_insight(
    df: pd.DataFrame, 
    keywords: List, 
    lang_analysis: Dict, 
    taxonomy_analysis: Dict,
    journey_analysis: Dict,
    expectation_anxiety: Dict,
    pos_pct: float,
    neg_pct: float
) -> str:
    """10ë…„ì°¨ ì´ìƒ ë§ˆì¼€í„°ì˜ ì „ëµì  ì¸ì‚¬ì´íŠ¸"""
    
    insights = []
    total = len(df)
    
    # 1. êµ¬ë§¤ ì—¬ì • ê¸°ë°˜ ì „ëµ
    insights.append("### ğŸ“Š êµ¬ë§¤ ì—¬ì • ê¸°ë°˜ ì „ëµ")
    
    if journey_analysis:
        awareness = journey_analysis.get('awareness', {}).get('percentage', 0)
        intent = journey_analysis.get('intent', {}).get('percentage', 0)
        experience = journey_analysis.get('experience', {}).get('percentage', 0)
        
        if awareness > 30:
            insights.append(f"â–¸ **ì¸ì§€ ë‹¨ê³„ ì§‘ì¤‘ ({awareness:.0f}%)**: ì•„ì§ ì œí’ˆ/ì„œë¹„ìŠ¤ë¥¼ ì²˜ìŒ ì ‘í•˜ëŠ” ì‹œì²­ìê°€ ë§ìŠµë‹ˆë‹¤. "
                          f"'ì´ê²Œ ë­”ì§€' ì„¤ëª…í•˜ëŠ” ì½˜í…ì¸ ê°€ íš¨ê³¼ì ì…ë‹ˆë‹¤. ë³µì¡í•œ ê¸°ëŠ¥ë³´ë‹¤ **í•µì‹¬ ê°€ì¹˜ 1ê°€ì§€**ë¥¼ ë°˜ë³µ ê°•ì¡°í•˜ì„¸ìš”.")
        
        if intent > 15:
            insights.append(f"â–¸ **êµ¬ë§¤ ì˜ë„ ì‹ í˜¸ ê°ì§€ ({intent:.0f}%)**: ì‹¤ì œ êµ¬ë§¤/ê°€ì…ì„ ê³ ë¯¼í•˜ëŠ” ì‹œì²­ìê°€ ìˆìŠµë‹ˆë‹¤. "
                          f"**CTA(Call-to-Action)ë¥¼ ëª…í™•íˆ** í•˜ê³ , ê°€ê²©/í˜œíƒ ì •ë³´ë¥¼ ì˜ìƒ ì„¤ëª…ë€ì— ì •ë¦¬í•˜ì„¸ìš”.")
        
        if experience > 20:
            insights.append(f"â–¸ **ê²½í—˜ì ì»¤ë®¤ë‹ˆí‹° ({experience:.0f}%)**: ì´ë¯¸ ì‚¬ìš©í•´ë³¸ ì‹œì²­ìê°€ ë§ìŠµë‹ˆë‹¤. "
                          f"ì´ë“¤ì„ **ë¦¬ë·°ì–´/ì•°ë°°ì„œë”**ë¡œ í™œìš©í•˜ë©´ ì‹ ë¢°ë„ ë†’ì€ 2ì°¨ ì½˜í…ì¸ ê°€ ìƒì„±ë©ë‹ˆë‹¤.")
    
    # 2. ê¸°ëŒ€ vs ë¶ˆì•ˆ ë°¸ëŸ°ìŠ¤
    insights.append("\n### âš–ï¸ ê¸°ëŒ€ vs ë¶ˆì•ˆ ìš”ì¸ ë°¸ëŸ°ìŠ¤")
    
    exp_count = expectation_anxiety.get('expectation_count', 0)
    anx_count = expectation_anxiety.get('anxiety_count', 0)
    
    if exp_count > anx_count * 2:
        insights.append(f"â–¸ **ê¸ì •ì  ê¸°ëŒ€ê° ìš°ì„¸**: ê¸°ëŒ€ í‘œí˜„({exp_count}ê±´)ì´ ë¶ˆì•ˆ í‘œí˜„({anx_count}ê±´)ì˜ 2ë°° ì´ìƒì…ë‹ˆë‹¤. "
                       f"ì´ ê¸°ëŒ€ê°ì„ **ì‚¬ì „ì˜ˆì•½, ì–¼ë¦¬ë²„ë“œ í˜œíƒ**ìœ¼ë¡œ ì „í™˜í•˜ì„¸ìš”.")
    elif anx_count > exp_count:
        insights.append(f"â–¸ **ë¶ˆì•ˆ ìš”ì¸ í•´ì†Œ í•„ìš”**: ë¶ˆì•ˆ/ìš°ë ¤ í‘œí˜„({anx_count}ê±´)ì´ ê¸°ëŒ€({exp_count}ê±´)ë³´ë‹¤ ë§ìŠµë‹ˆë‹¤. "
                       f"FAQ ì½˜í…ì¸ , íˆ¬ëª…í•œ ì •ë³´ ê³µê°œë¡œ **ì‹ ë¢° íšŒë³µ**ì´ ìš°ì„ ì…ë‹ˆë‹¤.")
    
    # ë¶ˆì•ˆ ìš”ì¸ ìƒ˜í”Œ
    if expectation_anxiety.get('anxiety_samples'):
        insights.append(f"â–¸ **ì£¼ìš” ë¶ˆì•ˆ í‚¤ì›Œë“œ**: {', '.join([s[:30] for s in expectation_anxiety['anxiety_samples'][:2]])}...")
    
    # 3. ì–¸ì–´ë³„ ë©”ì‹œì§€ ì „ëµ
    if lang_analysis and len(lang_analysis) > 1:
        insights.append("\n### ğŸŒ ì–¸ì–´ë³„ ë©”ì‹œì§€ ì „ëµ")
        
        for lang, data in list(lang_analysis.items())[:3]:
            if data['positive_pct'] > 70:
                insights.append(f"â–¸ **{lang}**: ë§¤ìš° ìš°í˜¸ì ({data['positive_pct']:.0f}% ê¸ì •). "
                               f"ì´ ì‹œì¥ì—ì„œëŠ” **íŒ¬ ì»¤ë®¤ë‹ˆí‹° êµ¬ì¶•**, í˜„ì§€ ì¸í”Œë£¨ì–¸ì„œ í˜‘ì—…ì´ íš¨ê³¼ì ì…ë‹ˆë‹¤.")
            elif data['negative_pct'] > 30:
                insights.append(f"â–¸ **{lang}**: ë¶€ì • ë¹„ìœ¨ ë†’ìŒ({data['negative_pct']:.0f}%). "
                               f"í•´ë‹¹ ì‹œì¥ì˜ **êµ¬ì²´ì  ë¶ˆë§Œ ìš”ì¸**ì„ íŒŒì•…í•˜ê³  í˜„ì§€í™”ëœ ëŒ€ì‘ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # 4. í•µì‹¬ í‚¤ì›Œë“œ í™œìš© ì „ëµ
    if keywords:
        insights.append("\n### ğŸ”‘ í‚¤ì›Œë“œ í™œìš© ì „ëµ")
        top_kw = keywords[0][0] if keywords else ""
        insights.append(f"â–¸ **ë©”ì¸ í‚¤ì›Œë“œ '{top_kw}'**: ì¸ë„¤ì¼, ì œëª©, ì²« 3ì´ˆì— ì´ í‚¤ì›Œë“œë¥¼ ë…¸ì¶œí•˜ë©´ CTR ìƒìŠ¹ì´ ì˜ˆìƒë©ë‹ˆë‹¤.")
        
        if len(keywords) >= 3:
            secondary = [k for k, _ in keywords[1:4]]
            insights.append(f"â–¸ **ì„œë¸Œ í‚¤ì›Œë“œ**: {', '.join(secondary)} - SEO íƒœê·¸ì™€ ì„¤ëª…ë€ì— í™œìš©í•˜ì„¸ìš”.")
        
        # í•´ì‹œíƒœê·¸ ì œì•ˆ
        hashtags = [f"#{k.replace(' ', '')}" for k, _ in keywords[:5]]
        insights.append(f"â–¸ **ì¶”ì²œ í•´ì‹œíƒœê·¸**: {' '.join(hashtags)}")
    
    # 5. ì¢…í•© ì „ëµ ë°©í–¥
    insights.append("\n### ğŸ¯ ì¢…í•© ì „ëµ ë°©í–¥")
    
    if pos_pct > 70:
        insights.append("â–¸ **ê³µê²©ì  í™•ì¥ ê°€ëŠ¥**: ê¸ì •ë¥ ì´ ë§¤ìš° ë†’ì•„ ë°”ì´ëŸ´ ë§ˆì¼€íŒ…, ìœ ë£Œ ê´‘ê³  í™•ëŒ€ê°€ ì í•©í•©ë‹ˆë‹¤. "
                       "ì§€ê¸ˆì´ **ì‹œì¥ ì ìœ ìœ¨ í™•ëŒ€**ì˜ ì ê¸°ì…ë‹ˆë‹¤.")
    elif pos_pct > 50:
        insights.append("â–¸ **ì ì§„ì  ì„±ì¥ ì „ëµ**: ê¸ì • ê¸°ë°˜ì´ ìˆìœ¼ë‚˜ ì—´ì„± íŒ¬ ì „í™˜ì´ í•„ìš”í•©ë‹ˆë‹¤. "
                       "**ì •ê¸° ì½˜í…ì¸  + ì»¤ë®¤ë‹ˆí‹° ê´€ë¦¬**ì— ì§‘ì¤‘í•˜ì„¸ìš”.")
    else:
        insights.append("â–¸ **ì‹ ë¢° êµ¬ì¶• ìš°ì„ **: í˜„ì¬ëŠ” ë¸Œëœë“œ ì¸ì§€ë„ì™€ ì‹ ë¢° êµ¬ì¶•ì´ ë¨¼ì €ì…ë‹ˆë‹¤. "
                       "**êµìœ¡ ì½˜í…ì¸ , íˆ¬ëª…í•œ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜**ìœ¼ë¡œ ê¸°ë°˜ì„ ë‹¤ì§€ì„¸ìš”.")
    
    return '\n\n'.join(insights)

# =============================================================================
# [NEW] ì˜ìƒ ì •ë³´ ë³´ì™„ ì²´í¬
# =============================================================================
def generate_video_info_suggestions(video_info: Dict, total_comments: int, pos_pct: float) -> List[Dict]:
    """ë¶„ì„ ì‹ ë¢°ë„ë¥¼ ìœ„í•œ ì¶”ê°€ ì •ë³´ ì œì•ˆ"""
    suggestions = []
    
    view_count = video_info.get('view_count', 0)
    like_count = video_info.get('like_count', 0)
    
    # 1. ì¡°íšŒìˆ˜ ëŒ€ë¹„ ëŒ“ê¸€ ë¹„ìœ¨
    if view_count > 0:
        comment_rate = total_comments / view_count * 100
        if comment_rate < 0.1:
            suggestions.append({
                'title': 'ëŒ“ê¸€ ì°¸ì—¬ìœ¨ ë‚®ìŒ',
                'desc': f'ì¡°íšŒìˆ˜ ëŒ€ë¹„ ëŒ“ê¸€ ë¹„ìœ¨ì´ {comment_rate:.3f}%ë¡œ ë‚®ìŠµë‹ˆë‹¤. ì‹œì²­ì ì°¸ì—¬ë¥¼ ìœ ë„í•˜ëŠ” CTAê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
                'type': 'warning'
            })
        elif comment_rate > 1:
            suggestions.append({
                'title': 'ë†’ì€ ì°¸ì—¬ë„',
                'desc': f'ëŒ“ê¸€ ì°¸ì—¬ìœ¨ {comment_rate:.2f}%ë¡œ ë§¤ìš° í™œë°œí•©ë‹ˆë‹¤. ë…¼ìŸì  ì£¼ì œì´ê±°ë‚˜ ì¶©ì„± íŒ¬ì¸µì´ ìˆìŠµë‹ˆë‹¤.',
                'type': 'info'
            })
    
    # 2. ì¢‹ì•„ìš”/ì¡°íšŒìˆ˜ ë¹„ìœ¨
    if view_count > 0 and like_count > 0:
        like_rate = like_count / view_count * 100
        if like_rate < 2:
            suggestions.append({
                'title': 'ì¢‹ì•„ìš” ì „í™˜ìœ¨ ì ê²€',
                'desc': f'ì¢‹ì•„ìš” ë¹„ìœ¨ {like_rate:.1f}%ëŠ” í‰ê·  ì´í•˜ì…ë‹ˆë‹¤. ì½˜í…ì¸  ë§Œì¡±ë„ ë˜ëŠ” CTA ìœ„ì¹˜ë¥¼ ì ê²€í•˜ì„¸ìš”.',
                'type': 'warning'
            })
    
    # 3. ê´‘ê³  ì—¬ë¶€ ì¶”ì •
    suggestions.append({
        'title': 'ê´‘ê³ /í˜‘ì°¬ ì—¬ë¶€ í™•ì¸ ê¶Œì¥',
        'desc': 'ëŒ“ê¸€ì—ì„œ ê´‘ê³  ê´€ë ¨ ë°˜ì‘ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. í˜‘ì°¬ ì½˜í…ì¸ ëŠ” ê°ì„± ë¶„ì„ í•´ì„ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
        'type': 'info'
    })
    
    # 4. ì—…ë¡œë“œ ì‹œì  ëŒ€ë¹„ ë°˜ì‘
    suggestions.append({
        'title': 'ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ë°˜ì‘ ë³€í™”',
        'desc': 'ì—…ë¡œë“œ ì§í›„ vs í˜„ì¬ ë°˜ì‘ ë¹„êµ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ˆê¸° ë°˜ì‘ê³¼ ì¥ê¸° ë°˜ì‘ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
        'type': 'info'
    })
    
    # 5. ë¶€ì •ë¥  ë†’ì„ ë•Œ
    if pos_pct < 50:
        suggestions.append({
            'title': 'ë¶€ì • ëŒ“ê¸€ ì›ì¸ ì‹¬ì¸µ ë¶„ì„',
            'desc': 'ê¸ì •ë¥ ì´ 50% ë¯¸ë§Œì…ë‹ˆë‹¤. ë¶€ì • ëŒ“ê¸€ì˜ êµ¬ì²´ì ì¸ ë¶ˆë§Œ ì‚¬í•­ì„ ë³„ë„ë¡œ ë¶„ë¥˜ ë¶„ì„í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.',
            'type': 'warning'
        })
    
    return suggestions

# =============================================================================
# ëŒ“ê¸€ ìˆ˜ì§‘
# =============================================================================
@st.cache_data(ttl=1800, show_spinner=False)
def collect_comments(url, max_comments):
    """
    ëŒ“ê¸€ ìˆ˜ì§‘ ê¸°ì¤€: YouTube APIì˜ 'top' (ì¸ê¸°ìˆœ) ì •ë ¬
    - ì¢‹ì•„ìš” ìˆ˜ê°€ ë§ì€ ëŒ“ê¸€ì´ ìš°ì„  ìˆ˜ì§‘ë¨
    - ìµœëŒ€ max_commentsê°œê¹Œì§€ë§Œ ìˆ˜ì§‘
    """
    import yt_dlp
    opts = {'quiet': True, 'no_warnings': True, 'extract_flat': False, 'getcomments': True,
            'extractor_args': {'youtube': {'max_comments': [str(max_comments)], 'comment_sort': ['top']}}}
    with yt_dlp.YoutubeDL(opts) as ydl:
        info = ydl.extract_info(url, download=False)
        if not info: return None, []
        
        total_comment_count = info.get('comment_count', 0) or 0
        
        video_info = {
            'title': info.get('title', 'ì œëª© ì—†ìŒ'),
            'channel': info.get('channel', info.get('uploader', '')),
            'thumbnail': info.get('thumbnail', ''),
            'view_count': info.get('view_count', 0),
            'like_count': info.get('like_count', 0),
            'comment_count': total_comment_count,  # ì „ì²´ ëŒ“ê¸€ ìˆ˜
            'upload_date': format_date(info.get('upload_date', '')),
            'upload_date_raw': info.get('upload_date', ''),
            'url': url,
        }
        raw = info.get('comments') or []
        comments = [{'text': c.get('text', ''), 'like_count': c.get('like_count', 0) or 0} for c in raw[:max_comments] if c]
        return video_info, comments

# =============================================================================
# ê°ì„± ë¶„ì„ (v8.1 ê°œì„ : ê¸ë¶€ì • ë¶„ë¥˜ ì •í™•ë„ í–¥ìƒ)
# =============================================================================
def analyze_sentiment(text):
    """
    ê°ì„± ë¶„ì„ ê¸°ì¤€:
    - ê¸ì •/ë¶€ì • ì´ëª¨ì§€ ë¹„ìœ¨
    - ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ë§¤ì¹­
    - ì›ƒìŒ í‘œí˜„ (ã…‹ã…‹, ã…ã…) â†’ ê¸ì • ê°€ì¤‘ì¹˜
    - ê°íƒ„/ê°•ì¡° í‘œí˜„ (!, í•˜, ì™€, ì˜¤) â†’ ë§¥ë½ì— ë”°ë¼ íŒë‹¨
    """
    if not text: return 'neutral', 0.0
    text_lower = text.lower()
    score = 0.0
    
    # 1. ì´ëª¨ì§€ ë¶„ì„
    pos_e = sum(1 for e in POSITIVE_EMOJIS if e in text)
    neg_e = sum(1 for e in NEGATIVE_EMOJIS if e in text)
    if pos_e + neg_e > 0: 
        score += (pos_e - neg_e) / (pos_e + neg_e + 1) * 1.5
    
    # 2. í‚¤ì›Œë“œ ë¶„ì„
    words = set(re.findall(r'[ê°€-í£]+|[a-z]+', text_lower))
    pos_w = sum(1 for w in words if any(pw in w or w in pw for pw in POSITIVE_WORDS))
    neg_w = sum(1 for w in words if any(nw in w or w in nw for nw in NEGATIVE_WORDS))
    if pos_w + neg_w > 0: 
        score += (pos_w - neg_w) / (pos_w + neg_w + 0.5)
    
    # 3. ì›ƒìŒ í‘œí˜„ (ã…‹ã…‹, ã…ã…) â†’ ê°•í•œ ê¸ì • ì‹ í˜¸
    laugh_pattern = re.findall(r'ã…‹{2,}|ã…{2,}|ã„±ã…‹+', text)
    if laugh_pattern:
        laugh_count = len(laugh_pattern)
        score += 0.4 * min(laugh_count, 3)  # ìµœëŒ€ 1.2ê¹Œì§€ ê°€ì¤‘ì¹˜
    
    # 4. ê°íƒ„ í‘œí˜„ ë¶„ì„ (ë§¥ë½ ê³ ë ¤)
    # "í•˜ ì§„ì§œ" ê°™ì€ í‘œí˜„ì€ ì›ƒìŒ í‘œí˜„ê³¼ í•¨ê»˜ ìˆìœ¼ë©´ ê¸ì •
    exclaim_pattern = re.search(r'^(í•˜|ì™€|ì˜¤|ìš°ì™€|í—|ëŒ€ë°•)\s', text)
    if exclaim_pattern:
        if laugh_pattern or pos_w > 0:  # ì›ƒìŒì´ë‚˜ ê¸ì • í‚¤ì›Œë“œì™€ í•¨ê»˜ë©´ ê¸ì •
            score += 0.3
        # ë¶€ì • í‚¤ì›Œë“œ ì—†ì´ ë‹¨ë…ì´ë©´ ì¤‘ë¦½ ìœ ì§€ (score ë³€ê²½ ì—†ìŒ)
    
    # 5. ëŠë‚Œí‘œ (ê¸ì • ë§¥ë½ì—ì„œë§Œ ê°€ì¤‘ì¹˜)
    exclamation_count = text.count('!')
    if exclamation_count >= 2 and neg_w == 0:
        score += 0.2
    
    # 6. ë¶€ì • íŒ¨í„´ (ëª…í™•í•œ ê²½ìš°ë§Œ)
    if re.search(r'ã…¡ã…¡+|;;+|\.\.\.+$', text) and pos_w == 0:
        score -= 0.3
    
    # 7. ìµœì¢… íŒì • (ì„ê³„ê°’ ì¡°ì •)
    if score > 0.15:  # ê¸ì • ì„ê³„ê°’ ì•½ê°„ ìƒí–¥
        return 'positive', min(score, 1.0)
    elif score < -0.2:  # ë¶€ì • ì„ê³„ê°’ ìƒí–¥ (ë” ëª…í™•í•´ì•¼ ë¶€ì •)
        return 'negative', max(score, -1.0)
    return 'neutral', score

# =============================================================================
# í‚¤ì›Œë“œ ì¶”ì¶œ
# =============================================================================
def extract_keywords(texts, top_n=15):
    words = []
    for t in texts:
        if t: words.extend(preprocess(str(t)).split())
    return Counter(words).most_common(top_n) if words else []

# =============================================================================
# ì°¨íŠ¸
# =============================================================================
def create_donut_chart(pos, neu, neg):
    colors = ['#1e3a5f', '#5a7fa8', '#a8c5de']
    fig = go.Figure(data=[go.Pie(
        values=[pos, neu, neg], labels=['ê¸ì •', 'ì¤‘ë¦½', 'ë¶€ì •'], hole=0.55,
        marker=dict(colors=colors), textinfo='percent', textfont=dict(size=13, color='white'),
        hovertemplate='%{label}: %{value}ê°œ<br>%{percent}<extra></extra>', sort=False
    )])
    fig.update_layout(
        showlegend=True, legend=dict(orientation='h', yanchor='bottom', y=-0.15, xanchor='center', x=0.5, font=dict(size=11)),
        margin=dict(t=20, b=40, l=20, r=20), height=260, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',
    )
    total = pos + neu + neg
    fig.add_annotation(text=f"<b>{total:,}</b><br><span style='font-size:10px;color:#64748b'>ëŒ“ê¸€</span>",
                      x=0.5, y=0.5, font=dict(size=16, color='#1e3a5f'), showarrow=False)
    return fig

def create_keyword_chart(keywords):
    if not keywords: return None
    kw_list = keywords[:10]
    labels = [k for k, _ in kw_list][::-1]
    values = [v for _, v in kw_list][::-1]
    n = len(labels)
    colors = [f'rgba(30, 58, 95, {0.3 + 0.7 * i / (n-1 if n > 1 else 1)})' for i in range(n)]
    
    fig = go.Figure(data=[go.Bar(
        x=values, y=labels, orientation='h', marker=dict(color=colors),
        text=values, textposition='outside', textfont=dict(size=10, color='#1e3a5f'),
        hovertemplate='%{y}: %{x}íšŒ<extra></extra>'
    )])
    fig.update_layout(
        margin=dict(t=20, b=20, l=10, r=40), height=260, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, tickfont=dict(size=11, color='#334155')), bargap=0.3,
    )
    return fig

def create_taxonomy_chart(taxonomy_analysis: Dict):
    """ì˜ê²¬ ìœ í˜• ë¶„í¬ ì°¨íŠ¸"""
    if not taxonomy_analysis:
        return None
    
    labels = [data['name'] for data in taxonomy_analysis.values()]
    values = [data['percentage'] for data in taxonomy_analysis.values()]
    colors = [data['color'] for data in taxonomy_analysis.values()]
    
    fig = go.Figure(data=[go.Bar(
        x=values, y=labels, orientation='h',
        marker=dict(color=colors),
        text=[f'{v:.0f}%' for v in values],
        textposition='outside',
        hovertemplate='%{y}: %{x:.1f}%<extra></extra>'
    )])
    fig.update_layout(
        margin=dict(t=20, b=20, l=10, r=50), height=220,
        paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False, range=[0, max(values)*1.3]),
        yaxis=dict(showgrid=False, tickfont=dict(size=11)),
        bargap=0.4,
    )
    return fig

def create_journey_chart(journey_analysis: Dict):
    """êµ¬ë§¤ ì—¬ì • í¼ë„ ì°¨íŠ¸"""
    if not journey_analysis:
        return None
    
    stages = ['awareness', 'interest', 'consideration', 'intent', 'experience']
    labels = [journey_analysis.get(s, {}).get('name', s) for s in stages]
    values = [journey_analysis.get(s, {}).get('percentage', 0) for s in stages]
    colors = [JOURNEY_STAGES[s]['color'] for s in stages]
    
    fig = go.Figure(data=[go.Funnel(
        y=labels, x=values,
        textinfo="value+percent total",
        marker=dict(color=['#3b82f6', '#10b981', '#f59e0b', '#ec4899', '#8b5cf6']),
        hovertemplate='%{y}: %{x:.1f}%<extra></extra>'
    )])
    fig.update_layout(
        margin=dict(t=20, b=20, l=10, r=10), height=280,
        paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',
    )
    return fig

# =============================================================================
# PDF ìƒì„± (ê³ ë„í™”)
# =============================================================================
def generate_pdf_report(
    video_info, total, pos, neu, neg, pos_pct, neg_pct,
    keywords, top_pos_comments, top_neg_comments,
    lang_analysis, taxonomy_analysis, journey_analysis,
    expectation_anxiety, marketing_insight, video_suggestions
):
    """ê³ ê¸‰ ë¶„ì„ì´ í¬í•¨ëœ PDF ë¦¬í¬íŠ¸"""
    from reportlab.lib import colors
    from reportlab.lib.pagesizes import A4
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import mm
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont
    import urllib.request
    import os
    
    # í•œê¸€ í°íŠ¸
    font_path = '/tmp/NotoSansKR-Regular.ttf'
    font_bold_path = '/tmp/NotoSansKR-Bold.ttf'
    
    if not os.path.exists(font_path):
        try:
            urllib.request.urlretrieve('https://github.com/google/fonts/raw/main/ofl/notosanskr/NotoSansKR-Regular.ttf', font_path)
            urllib.request.urlretrieve('https://github.com/google/fonts/raw/main/ofl/notosanskr/NotoSansKR-Bold.ttf', font_bold_path)
        except: pass
    
    try:
        pdfmetrics.registerFont(TTFont('NotoSansKR', font_path))
        pdfmetrics.registerFont(TTFont('NotoSansKR-Bold', font_bold_path))
        font_name, font_bold = 'NotoSansKR', 'NotoSansKR-Bold'
    except:
        font_name, font_bold = 'Helvetica', 'Helvetica-Bold'
    
    buffer = io.BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=15*mm, leftMargin=15*mm, topMargin=15*mm, bottomMargin=15*mm)
    
    styles = getSampleStyleSheet()
    styles.add(ParagraphStyle(name='KTitle', fontName=font_bold, fontSize=16, textColor=colors.HexColor('#1e3a5f'), spaceAfter=8))
    styles.add(ParagraphStyle(name='KHeading', fontName=font_bold, fontSize=11, textColor=colors.HexColor('#1e3a5f'), spaceBefore=12, spaceAfter=6))
    styles.add(ParagraphStyle(name='KBody', fontName=font_name, fontSize=9, textColor=colors.HexColor('#334155'), leading=14))
    styles.add(ParagraphStyle(name='KSmall', fontName=font_name, fontSize=8, textColor=colors.HexColor('#64748b'), leading=12))
    
    story = []
    
    # ì œëª©
    story.append(Paragraph("ìœ íŠœë¸Œ ëŒ“ê¸€ ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ v8.1", styles['KTitle']))
    story.append(Paragraph(f"ìƒì„±: {datetime.now().strftime('%Y-%m-%d %H:%M')}", styles['KSmall']))
    story.append(Spacer(1, 8))
    
    # ì˜ìƒ ì •ë³´
    story.append(Paragraph("ğŸ“º ì˜ìƒ ì •ë³´", styles['KHeading']))
    info_data = [
        ['ì œëª©', video_info.get('title', '')[:45] + ('...' if len(video_info.get('title', '')) > 45 else '')],
        ['ì±„ë„', video_info.get('channel', '')],
        ['ì—…ë¡œë“œ', video_info.get('upload_date', '')],
        ['ì¡°íšŒìˆ˜', format_num(video_info.get('view_count', 0))],
        ['ì¢‹ì•„ìš”', format_num(video_info.get('like_count', 0))],
    ]
    t = Table(info_data, colWidths=[50, 420])
    t.setStyle(TableStyle([('FONTNAME', (0,0), (-1,-1), font_name), ('FONTSIZE', (0,0), (-1,-1), 9),
                           ('TEXTCOLOR', (0,0), (0,-1), colors.HexColor('#64748b')), ('BOTTOMPADDING', (0,0), (-1,-1), 4)]))
    story.append(t)
    story.append(Spacer(1, 8))
    
    # í•µì‹¬ ì§€í‘œ
    story.append(Paragraph("ğŸ“Š ê°ì„± ë¶„ì„", styles['KHeading']))
    sent_data = [['ë¶„ì„ ëŒ“ê¸€', f'{total:,}ê°œ', 'ê¸ì •ë¥ ', f'{pos_pct:.1f}%', 'ë¶€ì •ë¥ ', f'{neg_pct:.1f}%']]
    t = Table(sent_data, colWidths=[55, 60, 45, 50, 45, 50])
    t.setStyle(TableStyle([('FONTNAME', (0,0), (-1,-1), font_name), ('FONTSIZE', (0,0), (-1,-1), 9),
                           ('BACKGROUND', (0,0), (-1,-1), colors.HexColor('#f8fafc')), ('BOTTOMPADDING', (0,0), (-1,-1), 6)]))
    story.append(t)
    story.append(Spacer(1, 8))
    
    # í‚¤ì›Œë“œ
    story.append(Paragraph("ğŸ”‘ í•µì‹¬ í‚¤ì›Œë“œ", styles['KHeading']))
    if keywords:
        kw_text = ', '.join([f"{k}({v})" for k, v in keywords[:10]])
        story.append(Paragraph(kw_text, styles['KBody']))
    story.append(Spacer(1, 8))
    
    # ì–¸ì–´ë³„ ë¶„ì„
    if lang_analysis:
        story.append(Paragraph("ğŸŒ ì–¸ì–´ë³„ ë°˜ì‘", styles['KHeading']))
        for lang, data in list(lang_analysis.items())[:4]:
            story.append(Paragraph(f"â€¢ {lang}: {data['count']}ê±´ ({data['percentage']:.0f}%) | ê¸ì • {data['positive_pct']:.0f}%", styles['KBody']))
        story.append(Spacer(1, 8))
    
    # ì˜ê²¬ ìœ í˜•
    if taxonomy_analysis:
        story.append(Paragraph("ğŸ§© ì˜ê²¬ ìœ í˜•ë³„ ë¶„í¬", styles['KHeading']))
        for cat_id, data in taxonomy_analysis.items():
            story.append(Paragraph(f"â€¢ {data['name']}: {data['percentage']:.0f}% (ê¸ì • {data['positive_pct']:.0f}%)", styles['KBody']))
        story.append(Spacer(1, 8))
    
    # êµ¬ë§¤ ì—¬ì •
    if journey_analysis:
        story.append(Paragraph("ğŸ›’ êµ¬ë§¤ ì—¬ì • ë¶„í¬", styles['KHeading']))
        for stage_id in ['awareness', 'interest', 'consideration', 'intent', 'experience']:
            data = journey_analysis.get(stage_id, {})
            if data.get('percentage', 0) > 0:
                story.append(Paragraph(f"â€¢ {data.get('name', stage_id)}: {data.get('percentage', 0):.0f}%", styles['KBody']))
        story.append(Spacer(1, 8))
    
    # ê¸°ëŒ€ vs ë¶ˆì•ˆ
    story.append(Paragraph("âš–ï¸ ê¸°ëŒ€ vs ë¶ˆì•ˆ ìš”ì¸", styles['KHeading']))
    story.append(Paragraph(f"â€¢ ê¸°ëŒ€ í‘œí˜„: {expectation_anxiety.get('expectation_count', 0)}ê±´", styles['KBody']))
    story.append(Paragraph(f"â€¢ ë¶ˆì•ˆ í‘œí˜„: {expectation_anxiety.get('anxiety_count', 0)}ê±´", styles['KBody']))
    story.append(Spacer(1, 8))
    
    # ë§ˆì¼€íŒ… ì „ëµ ì¸ì‚¬ì´íŠ¸
    story.append(PageBreak())
    story.append(Paragraph("ğŸ¯ ë§ˆì¼€íŒ… ì „ëµ ì¸ì‚¬ì´íŠ¸", styles['KHeading']))
    if marketing_insight:
        clean_insight = marketing_insight.replace('**', '').replace('###', 'â– ').replace('â–¸', 'â€¢')
        for para in clean_insight.split('\n\n'):
            if para.strip():
                story.append(Paragraph(para.strip()[:300], styles['KBody']))
                story.append(Spacer(1, 3))
    story.append(Spacer(1, 8))
    
    # ì¶”ê°€ í•„ìš” ì •ë³´
    story.append(Paragraph("ğŸ“Œ ë¶„ì„ ë³´ì™„ ì œì•ˆ", styles['KHeading']))
    for sug in video_suggestions[:3]:
        story.append(Paragraph(f"â€¢ {sug['title']}: {sug['desc'][:80]}...", styles['KSmall']))
    story.append(Spacer(1, 8))
    
    # ì£¼ìš” ëŒ“ê¸€
    story.append(Paragraph("ğŸ’¬ ì£¼ìš” ê¸ì • ëŒ“ê¸€", styles['KHeading']))
    for c in top_pos_comments[:2]:
        story.append(Paragraph(f'"{c["text"][:80]}..." (ğŸ‘{c["like_count"]:,})', styles['KSmall']))
    
    story.append(Paragraph("ğŸ’¬ ì£¼ìš” ë¶€ì • ëŒ“ê¸€", styles['KHeading']))
    for c in top_neg_comments[:2]:
        story.append(Paragraph(f'"{c["text"][:80]}..." (ğŸ‘{c["like_count"]:,})', styles['KSmall']))
    
    # í‘¸í„°
    story.append(Spacer(1, 15))
    story.append(Paragraph("â”€" * 60, styles['KSmall']))
    story.append(Paragraph("ìœ íŠœë¸Œ ëŒ“ê¸€ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ê¸° v8.1 | ìë™ ìƒì„± ë¦¬í¬íŠ¸", styles['KSmall']))
    
    doc.build(story)
    buffer.seek(0)
    return buffer

# =============================================================================
# ë©”ì¸ ì•±
# =============================================================================
def main():
    # í—¤ë”
    st.markdown('''
    <div class="header">
        <h1>ğŸ“Š ìœ íŠœë¸Œ ëŒ“ê¸€ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ê¸°</h1>
        <p>AI ê¸°ë°˜ ê³ ê¸‰ ëŒ“ê¸€ ë¶„ì„ Â· ë§ˆì¼€íŒ… ì „ëµ ì¸ì‚¬ì´íŠ¸</p>
    </div>
    ''', unsafe_allow_html=True)
    
    # ì…ë ¥
    col1, col2, col3 = st.columns([1, 2.5, 1])
    with col2:
        url = st.text_input("URL", placeholder="https://www.youtube.com/watch?v=...", 
                           label_visibility="collapsed", key=f"url_input_{st.session_state.input_key}")
        col_btn1, col_btn2 = st.columns([2, 1])
        with col_btn1:
            btn = st.button("ğŸ” ë¶„ì„ ì‹œì‘", use_container_width=True)
        with col_btn2:
            if st.button("ğŸ”„ ì´ˆê¸°í™”", use_container_width=True):
                st.session_state.input_key += 1
                st.rerun()
    
    if btn and url:
        vid = extract_video_id(url)
        if not vid:
            st.error("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ URLì…ë‹ˆë‹¤.")
            return
        
        try:
            # =====================================================================
            # ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
            # =====================================================================
            progress = st.progress(0, "ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...")
            video_info, comments = collect_comments(url, CONFIG["max_comments"])
            
            if not video_info or not comments:
                st.warning("âš ï¸ ëŒ“ê¸€ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            progress.progress(20, "ê°ì„± ë¶„ì„ ì¤‘...")
            df = pd.DataFrame(comments)
            results = [analyze_sentiment(str(t)) for t in df['text'].fillna('')]
            df['sentiment'] = [r[0] for r in results]
            
            progress.progress(40, "ì–¸ì–´ ê°ì§€ ì¤‘...")
            df['language'] = df['text'].apply(detect_language)
            
            progress.progress(50, "ì˜ê²¬ ìœ í˜• ë¶„ë¥˜ ì¤‘...")
            df['categories'] = df['text'].apply(classify_opinion_type)
            
            progress.progress(60, "êµ¬ë§¤ ì—¬ì • ë¶„ì„ ì¤‘...")
            df['journey_stage'] = df['text'].apply(classify_journey_stage)
            
            progress.progress(70, "í‚¤ì›Œë“œ ë¶„ì„ ì¤‘...")
            keywords = extract_keywords(df['text'].tolist(), CONFIG["top_keywords_count"])
            
            progress.progress(80, "ê³ ê¸‰ ë¶„ì„ ì¤‘...")
            lang_analysis = analyze_by_language(df)
            taxonomy_analysis = analyze_opinion_taxonomy(df)
            journey_analysis = analyze_journey_stages(df)
            expectation_anxiety = analyze_expectation_anxiety(df)
            
            progress.progress(90, "ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘...")
            
            # ê¸°ë³¸ í†µê³„
            total = len(df)
            pos = int((df['sentiment'] == 'positive').sum())
            neu = int((df['sentiment'] == 'neutral').sum())
            neg = int((df['sentiment'] == 'negative').sum())
            pos_pct = pos / total * 100 if total else 0
            neg_pct = neg / total * 100 if total else 0
            
            # ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸ ìƒì„±
            marketing_insight = generate_marketing_strategy_insight(
                df, keywords, lang_analysis, taxonomy_analysis, 
                journey_analysis, expectation_anxiety, pos_pct, neg_pct
            )
            
            # ì˜ìƒ ì •ë³´ ë³´ì™„ ì œì•ˆ
            video_suggestions = generate_video_info_suggestions(video_info, total, pos_pct)
            
            # ì£¼ìš” ëŒ“ê¸€
            top_pos_df = df[df['sentiment'] == 'positive'].nlargest(3, 'like_count')
            top_neg_df = df[df['sentiment'] == 'negative'].nlargest(3, 'like_count')
            top_pos_comments = top_pos_df.to_dict('records')
            top_neg_comments = top_neg_df.to_dict('records')
            
            progress.progress(100, "ì™„ë£Œ!")
            progress.empty()
            
            # =====================================================================
            # UI ë Œë”ë§
            # =====================================================================
            
            # ì˜ìƒ ì •ë³´ ë°•ìŠ¤
            st.markdown('<div class="video-info-box">', unsafe_allow_html=True)
            c1, c2 = st.columns([1, 2.5])
            with c1:
                if video_info.get('thumbnail'):
                    st.image(video_info['thumbnail'], use_container_width=True)
            with c2:
                st.markdown(f"### {video_info.get('title', '')}")
                st.markdown(f'''
                <div class="video-info-row"><span class="video-info-label">ì±„ë„ëª…</span><span class="video-info-value">{video_info.get('channel', '')}</span></div>
                <div class="video-info-row"><span class="video-info-label">ì—…ë¡œë“œ ë‚ ì§œ</span><span class="video-info-value">{video_info.get('upload_date', '')}</span></div>
                <div class="video-info-row"><span class="video-info-label">ì¡°íšŒìˆ˜</span><span class="video-info-value">{format_num(video_info.get('view_count', 0))}</span></div>
                <div class="video-info-row"><span class="video-info-label">ì¢‹ì•„ìš”</span><span class="video-info-value">{format_num(video_info.get('like_count', 0))}</span></div>
                ''', unsafe_allow_html=True)
            st.markdown('</div>', unsafe_allow_html=True)
            
            # í•µì‹¬ ì§€í‘œ
            total_comments = video_info.get('comment_count', 0)
            
            c1, c2, c3, c4, c5 = st.columns(5)
            if total_comments > CONFIG["max_comments"]:
                c1.metric("ë¶„ì„ ëŒ“ê¸€", f"{total:,}ê°œ", delta=f"ì „ì²´ {format_num(total_comments)}ê°œ ì¤‘")
            else:
                c1.metric("ë¶„ì„ ëŒ“ê¸€", f"{total:,}ê°œ")
            c2.metric("ê¸ì •ë¥ ", f"{pos_pct:.1f}%")
            c3.metric("ë¶€ì •ë¥ ", f"{neg_pct:.1f}%")
            c4.metric("ì–¸ì–´ ìˆ˜", f"{len(lang_analysis)}ê°œ")
            c5.metric("ì˜ê²¬ ìœ í˜•", f"{len(taxonomy_analysis)}ê°œ")
            
            # 1000ê°œ ì´ˆê³¼ ì‹œ ì•ˆë‚´ ë¬¸êµ¬
            if total_comments > CONFIG["max_comments"]:
                st.info(f"â„¹ï¸ ì „ì²´ ëŒ“ê¸€ {format_num(total_comments)}ê°œ ì¤‘ **ì¢‹ì•„ìš”(ì¸ê¸°) ìˆœìœ¼ë¡œ ìƒìœ„ {CONFIG['max_comments']:,}ê°œ**ë§Œ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì¸ê¸° ëŒ“ê¸€ ìœ„ì£¼ì˜ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.")
            
            # PDF ë‹¤ìš´ë¡œë“œ
            try:
                pdf_buffer = generate_pdf_report(
                    video_info, total, pos, neu, neg, pos_pct, neg_pct,
                    keywords, top_pos_comments, top_neg_comments,
                    lang_analysis, taxonomy_analysis, journey_analysis,
                    expectation_anxiety, marketing_insight, video_suggestions
                )
                st.download_button("ğŸ“„ PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ", data=pdf_buffer,
                                  file_name=f"youtube_insight_{datetime.now().strftime('%Y%m%d_%H%M')}.pdf",
                                  mime="application/pdf")
            except Exception as e:
                st.caption(f"PDF ìƒì„± ë¶ˆê°€: {e}")
            
            # =====================================================================
            # ğŸ“Š ê¸°ë³¸ ë¶„ì„
            # =====================================================================
            st.markdown('<div class="section-title">ğŸ“Š ê°ì„± ë¶„ì„ ê²°ê³¼</div>', unsafe_allow_html=True)
            c1, c2 = st.columns(2)
            with c1:
                st.markdown('<div class="card">', unsafe_allow_html=True)
                st.markdown("**ê°ì„± ë¶„í¬**")
                st.plotly_chart(create_donut_chart(pos, neu, neg), use_container_width=True, config={'displayModeBar': False})
                st.caption("ğŸ“Œ ë¶„ì„ ê¸°ì¤€: ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ, ì´ëª¨ì§€, ì›ƒìŒ í‘œí˜„(ã…‹ã…‹, ã…ã…) ë“±ì„ ì¢…í•© ë¶„ì„")
                st.markdown('</div>', unsafe_allow_html=True)
            with c2:
                st.markdown('<div class="card">', unsafe_allow_html=True)
                st.markdown("**í•µì‹¬ í‚¤ì›Œë“œ**")
                if keywords:
                    st.plotly_chart(create_keyword_chart(keywords), use_container_width=True, config={'displayModeBar': False})
                st.caption("ğŸ“Œ ìˆ«ì = í•´ë‹¹ í‚¤ì›Œë“œê°€ ëŒ“ê¸€ì—ì„œ ì–¸ê¸‰ëœ íšŸìˆ˜")
                st.markdown('</div>', unsafe_allow_html=True)
            
            # =====================================================================
            # ğŸ’¬ ì£¼ìš” ëŒ“ê¸€ (ê°ì„± ë¶„ì„ ë°”ë¡œ ë‹¤ìŒìœ¼ë¡œ ì´ë™)
            # =====================================================================
            st.markdown('<div class="section-title">ğŸ’¬ ì£¼ìš” ëŒ“ê¸€</div>', unsafe_allow_html=True)
            c1, c2 = st.columns(2)
            
            with c1:
                st.markdown("**ğŸ‘ ê¸ì • TOP 3**")
                for c in top_pos_comments:
                    txt = str(c['text'])[:100] + ('...' if len(str(c['text'])) > 100 else '')
                    st.markdown(f'''<div class="comment pos">
                        <div class="comment-text">"{txt}"</div>
                        <div class="comment-likes">ğŸ‘ {int(c['like_count']):,}</div>
                    </div>''', unsafe_allow_html=True)
            
            with c2:
                st.markdown("**ğŸ‘ ë¶€ì • TOP 3**")
                if top_neg_comments:
                    for c in top_neg_comments:
                        txt = str(c['text'])[:100] + ('...' if len(str(c['text'])) > 100 else '')
                        st.markdown(f'''<div class="comment neg">
                            <div class="comment-text">"{txt}"</div>
                            <div class="comment-likes">ğŸ‘ {int(c['like_count']):,}</div>
                        </div>''', unsafe_allow_html=True)
                else:
                    st.success("ğŸ‰ ë¶€ì • ëŒ“ê¸€ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤!")
            
            # =====================================================================
            # ğŸŒ ì–¸ì–´ë³„ ë¶„ì„
            # =====================================================================
            if lang_analysis:
                st.markdown('<div class="section-title">ğŸŒ ì–¸ì–´ë³„ ë¶„ì„</div>', unsafe_allow_html=True)
                
                # ì–¸ì–´ íƒœê·¸
                lang_tags = ' '.join([f'<span class="lang-tag{"" if i > 0 else " primary"}">{lang} ({data["percentage"]:.0f}%)</span>' 
                                     for i, (lang, data) in enumerate(sorted(lang_analysis.items(), key=lambda x: -x[1]['count']))])
                st.markdown(f'<div style="margin-bottom:1rem">{lang_tags}</div>', unsafe_allow_html=True)
                
                # ì–¸ì–´ë³„ ìƒì„¸
                cols = st.columns(min(len(lang_analysis), 3))
                for i, (lang, data) in enumerate(list(lang_analysis.items())[:3]):
                    with cols[i]:
                        st.markdown(f'''<div class="card">
                            <div style="font-weight:600;color:#1e3a5f;margin-bottom:0.5rem">{lang}</div>
                            <div style="font-size:0.85rem;color:#475569">
                                ëŒ“ê¸€ {data['count']:,}ê°œ ({data['percentage']:.0f}%)<br>
                                ê¸ì • {data['positive_pct']:.0f}% Â· ë¶€ì • {data['negative_pct']:.0f}%<br>
                                í‚¤ì›Œë“œ: {', '.join([k for k, _ in data['keywords'][:3]])}
                            </div>
                        </div>''', unsafe_allow_html=True)
                
                # ì¸ì‚¬ì´íŠ¸
                lang_insight = generate_language_insight(lang_analysis)
                if lang_insight:
                    st.markdown(f'<div class="insight"><div class="insight-desc">{lang_insight}</div></div>', unsafe_allow_html=True)
            
            # =====================================================================
            # ğŸ§© ì˜ê²¬ ìœ í˜•ë³„ ë¶„ì„
            # =====================================================================
            if taxonomy_analysis:
                st.markdown('<div class="section-title">ğŸ§© ëŒ“ê¸€ ìœ í˜•ë³„ ë¶„ì„</div>', unsafe_allow_html=True)
                
                c1, c2 = st.columns([1.2, 1])
                with c1:
                    st.markdown('<div class="card">', unsafe_allow_html=True)
                    st.markdown("**ì˜ê²¬ ìœ í˜• ë¶„í¬**")
                    fig = create_taxonomy_chart(taxonomy_analysis)
                    if fig:
                        st.plotly_chart(fig, use_container_width=True, config={'displayModeBar': False})
                    st.markdown('</div>', unsafe_allow_html=True)
                
                with c2:
                    st.markdown("**ìœ í˜•ë³„ ëŒ€í‘œ ì˜ê²¬**")
                    for cat_id, data in list(taxonomy_analysis.items())[:3]:
                        if data['sample_comment']:
                            st.markdown(f'''<div class="category-card" style="border-color:{data['color']}">
                                <div class="category-title">{data['name']}</div>
                                <div class="category-pct">{data['percentage']:.0f}% Â· ê¸ì • {data['positive_pct']:.0f}%</div>
                                <div class="category-sample">"{data['sample_comment'][:60]}..."</div>
                            </div>''', unsafe_allow_html=True)
                
                # ì¸ì‚¬ì´íŠ¸
                taxonomy_insight = generate_taxonomy_insight(taxonomy_analysis)
                if taxonomy_insight:
                    st.markdown(f'<div class="insight"><div class="insight-desc">{taxonomy_insight}</div></div>', unsafe_allow_html=True)
            
            # =====================================================================
            # ğŸ¯ ë§ˆì¼€íŒ… ì „ëµ ì¸ì‚¬ì´íŠ¸
            # =====================================================================
            st.markdown('<div class="section-title">ğŸ¯ ë§ˆì¼€íŒ… ì „ëµ ì¸ì‚¬ì´íŠ¸</div>', unsafe_allow_html=True)
            
            # êµ¬ë§¤ ì—¬ì • í¼ë„
            c1, c2 = st.columns([1, 1.5])
            with c1:
                st.markdown('<div class="card">', unsafe_allow_html=True)
                st.markdown("**êµ¬ë§¤ ì—¬ì • ë¶„í¬**")
                journey_fig = create_journey_chart(journey_analysis)
                if journey_fig:
                    st.plotly_chart(journey_fig, use_container_width=True, config={'displayModeBar': False})
                st.markdown('</div>', unsafe_allow_html=True)
            
            with c2:
                st.markdown('<div class="card">', unsafe_allow_html=True)
                st.markdown("**ê¸°ëŒ€ vs ë¶ˆì•ˆ ìš”ì¸**")
                exp_c = expectation_anxiety.get('expectation_count', 0)
                anx_c = expectation_anxiety.get('anxiety_count', 0)
                
                col_a, col_b = st.columns(2)
                with col_a:
                    st.markdown(f'''<div class="stat-box">
                        <div class="stat-value">{exp_c}</div>
                        <div class="stat-label">ê¸°ëŒ€ í‘œí˜„</div>
                    </div>''', unsafe_allow_html=True)
                with col_b:
                    st.markdown(f'''<div class="stat-box-light">
                        <div class="stat-value">{anx_c}</div>
                        <div class="stat-label">ë¶ˆì•ˆ í‘œí˜„</div>
                    </div>''', unsafe_allow_html=True)
                
                if expectation_anxiety.get('anxiety_samples'):
                    st.markdown("**ì£¼ìš” ë¶ˆì•ˆ í‚¤ì›Œë“œ:**")
                    for sample in expectation_anxiety['anxiety_samples'][:2]:
                        st.caption(f'"{sample[:50]}..."')
                st.markdown('</div>', unsafe_allow_html=True)
            
            # ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸ ìƒì„¸
            if marketing_insight:
                st.markdown('<div class="card">', unsafe_allow_html=True)
                st.markdown(marketing_insight)
                st.markdown('</div>', unsafe_allow_html=True)
            
            # =====================================================================
            # ğŸ“Œ ì¶”ê°€ í•„ìš” ì •ë³´
            # =====================================================================
            st.markdown('<div class="section-title">ğŸ“Œ ë¶„ì„ ë³´ì™„ì„ ìœ„í•œ ì¶”ê°€ ì •ë³´</div>', unsafe_allow_html=True)
            
            for sug in video_suggestions:
                box_class = "warning-box" if sug['type'] == 'warning' else "info-box"
                st.markdown(f'''<div class="{box_class}">
                    <strong>{sug['title']}</strong><br>
                    <span style="font-size:0.9rem">{sug['desc']}</span>
                </div>''', unsafe_allow_html=True)
            
            st.markdown('<div class="footer">ìœ íŠœë¸Œ ëŒ“ê¸€ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ê¸° v8.1</div>', unsafe_allow_html=True)
            
        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜: {str(e)}")
            import traceback
            st.code(traceback.format_exc())

if __name__ == "__main__":
    main()
